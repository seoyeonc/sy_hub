[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seoyeon",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 2, 2023\n\n\nGraph Shift Operator\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nGraph and Spectral domain\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean data of GODE\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean vs Euclidean\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nGraph Signal\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nRegular Graph\n\n\nSEOYEON CHOI\n\n\n\n\nMay 18, 2023\n\n\nEbayesThresh Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nMay 18, 2023\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬\n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬\n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_ml.html",
    "href": "2_ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "3_gode.html",
    "href": "3_gode.html",
    "title": "GODE",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 1, 2023\n\n\nGraph and Spectral domain\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean data of GODE\n\n\nSEOYEON CHOI\n\n\n\n\nMay 18, 2023\n\n\nEbayesThresh Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html",
    "href": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "using LinearAlgebra, DSP"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "href": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Simultaneously Diagonalizable",
    "text": "Simultaneously Diagonalizable\n매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 대각화 가능하다는 것은 아래의 표현을 만족하는 적당한 invertible matrix \\({\\bf \\Psi}_A\\), \\({\\bf \\Psi}_B\\)와 대각행렬 \\({\\bf \\Lambda}_A\\), \\({\\bf \\Lambda}_B\\)가 존재한다는 의미가 된다.\n\\[{\\bf A} = {\\bf V}_{A} {\\bf \\Lambda}_A {\\bf V}_{A}^{-1}\\]\n\\[{\\bf B} = {\\bf V}_{B} {\\bf \\Lambda}_B {\\bf V}_{B}^{-1}\\]\n그리고 만약에 \\({\\bf V}_{A}={\\bf V}_{B}\\)이라면 즉\n\\[{\\bf A} = {\\bf V} {\\bf \\Lambda}_A {\\bf V}^{-1}\\]\n\\[{\\bf B} = {\\bf V} {\\bf \\Lambda}_B {\\bf V}^{-1}\\]\n이라면 \\(\\{{\\bf A},{\\bf B}\\}\\)가 simultaneously diagonalzable 하다고 표현한다."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#commute",
    "href": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#commute",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Commute",
    "text": "Commute\n두 matrix \\({\\bf A}\\)와 \\({\\bf B}\\)에 대하여\n\\[{\\bf A}{\\bf B}= {\\bf B}{\\bf A}\\]\n인 관계가 성립하면 두 매트릭스가 commute 한다고 표현한다. 그런데 \\({\\bf A}{\\bf B}={\\bf A}{\\bf B}\\)의 조건은 \\({\\bf A}, {\\bf B}\\)가 동시대각화가능할 (simultaneously diagonalzable) 조건과 같다. 1 따라서 simultaneously diagonalzable 는 commute와 같은 말이라 생각해도 무방하다.1 필요충분조건이다.\n\n참고: 위키피디아.."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "href": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Shift Invariant Filter",
    "text": "Shift Invariant Filter\n\nref: Djuric and Richard (2018) Chap 8.3 의 내용 중 일부\n\nDjuric, Petar, and Cédric Richard. 2018. Cooperative and Graph Signal Processing: Principles and Applications. Academic Press.\n\nDefine the matrix \\({\\bf B}\\) as periodic shift matrix such that\n\\[\n{\\bf B} = \\begin{bmatrix}\n0 & 0 & 0 & \\dots  & 0 & 1 \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\n0 & 0 & \\dots & 1 & 0 & 0 \\\\\n0 & 0 & \\dots & 0 & 1 & 0 \\\\\n\\end{bmatrix}.\\]\nA generic filter \\({\\boldsymbol h}\\) is given by its \\(z\\)-transform\n\\[h(z)=h_0z^0+h_1z^{-1}+\\cdots +h_{N-1}z^{-(N-1)}\\]\nwhere \\(s_{n-1}=z^{-1}s_n\\). In vector notation, and with respect to the standard basis \\({\\bf I}\\), the filter is represented by the matrix \\({\\bf H}\\), a polynomial in the cyclic shift\n\\[{\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+\\cdots+h_{N-1}{\\bf B}^{N-1}.\\]\nFilters are shift invariant iff\n\\[z\\cdot h(z) = h(z)\\cdot z\\]\nor from the matrix representation\n\\[{\\bf B}h({\\bf B})=h({\\bf B}){\\bf B}.\\]\nExample\nLet \\({\\bf B}\\) as\n\nB= [0 1 0 0 0 0 0\n    0 0 1 0 0 0 0 \n    0 0 0 1 0 0 0 \n    0 0 0 0 1 0 0 \n    0 0 0 0 0 1 0 \n    0 0 0 0 0 0 1 \n    1 0 0 0 0 0 0]\n\n7×7 Matrix{Int64}:\n 0  1  0  0  0  0  0\n 0  0  1  0  0  0  0\n 0  0  0  1  0  0  0\n 0  0  0  0  1  0  0\n 0  0  0  0  0  1  0\n 0  0  0  0  0  0  1\n 1  0  0  0  0  0  0\n\n\nDefine \\({\\boldsymbol h}\\) as\n\nh = [1/3,1/3,1/3]\n\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\n\nFurthermore define \\({\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+h_2{\\bf B}^2\\)\n\nH = (1/3)*B^0 + (1/3)*B^1 + (1/3)*B^2 \n\n7×7 Matrix{Float64}:\n 0.333333  0.333333  0.333333  0.0       0.0       0.0       0.0\n 0.0       0.333333  0.333333  0.333333  0.0       0.0       0.0\n 0.0       0.0       0.333333  0.333333  0.333333  0.0       0.0\n 0.0       0.0       0.0       0.333333  0.333333  0.333333  0.0\n 0.0       0.0       0.0       0.0       0.333333  0.333333  0.333333\n 0.333333  0.0       0.0       0.0       0.0       0.333333  0.333333\n 0.333333  0.333333  0.0       0.0       0.0       0.0       0.333333\n\n\nObserve following:\n\nB*H == H*B \n\ntrue\n\n\nThus, filter \\({\\boldsymbol h}\\) is shift invariant filter and matrix \\({\\bf H}\\) is shift invariant operator.\nnote: \\({\\boldsymbol h}\\) is moving average filter.\nnote: for any \\({\\bf x}\\), \\({\\bf H}{\\bf x}\\) is definded by\n\\[\\left[\\frac{x_{n-1}+x_n+x_1}{3},\\frac{x_n+x_1+x_2}{3},\\dots,\\frac{x_{n-3}+x_{n-2}+x_n}{3}\\right].\\]\n\nx = [1,1,1,1,2,2,2]\nH*x\n\n7-element Vector{Float64}:\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666665\n 2.0\n 1.6666666666666665\n 1.3333333333333333\n\n\nnote: In some sense, the matrix \\({\\bf H}{\\bf x}\\) can be thought as generalized version of \\({\\boldsymbol h}\\star {\\bf x}\\) where \\(\\star\\) is convolution up to shift\n\nconv(h,x)\n\n9-element Vector{Float64}:\n 0.3333333333333334\n 0.6666666666666667\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666667\n 2.0\n 1.3333333333333333\n 0.6666666666666667\n\n\nFinally, we observe that, from the Cayley-Hamilton Theorem, \\({\\bf B}\\) satisfies its characteristic polynomial \\(\\Delta({\\bf B})\\), where \\(\\Delta(\\lambda)\\) is the determinant of \\(\\lambda{\\bf I}-{\\bf B}\\). The characteristic polynomial \\(\\Delta({\\bf B})\\) has degree \\(N\\), so, in DSP, as described so far, linear filters are (matrix) polynomial with degree at most \\(N-1\\).\n\n이 부분은 책에 써있길래 가져오긴 했는데, 무슨 의미인지 모르겠음"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "href": "posts/2_Studies/GRAPH/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Coexisting Approaches",
    "text": "Coexisting Approaches\nStationary graph processes were first defined and analyzed in (Girault 2015). The fundamental problem identified there is that GSOs do not preserve energy in general and therefore cannot be isometric (Gavili and Zhang 2017). This problem is addressed in (Girault, Gonçalves, and Fleury 2015) with the definition of an isometric graph shift that preserves the eigenvector space of the Laplacian GSO but modifies its eigenvalues.\n\nGirault, Benjamin. 2015. “Stationary Graph Signals Using an Isometric Graph Translation.” In 2015 23rd European Signal Processing Conference (EUSIPCO), 1516–20. IEEE.\n\nGavili, Adnan, and Xiao-Ping Zhang. 2017. “On the Shift Operator, Graph Frequency, and Optimal Filtering in Graph Signal Processing.” IEEE Transactions on Signal Processing 65 (23): 6303–18.\n\nGirault, Benjamin, Paulo Gonçalves, and Éric Fleury. 2015. “Translation on Graphs: An Isometric Shift Operator.” IEEE Signal Processing Letters 22 (12): 2416–20.\nA stationary graph process is then defined as one whose probability distributions are invariant with respect to multiplications with the isometric shift. One drawback of this approach is that the isometric shift is a complex-valued operator and has a sparsity structure (if any) different from \\({\\bf S}\\). By contrast, the vertex-based definition in\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf x}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf x}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf x}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf x} \\Big)^H \\bigg]\\]\nis based on the original GSO \\({\\bf S}\\), which is local and real-valued. As a result, above Eq. provides intuition on the relations between stationarity and locality, which can be leveraged to develop stationarity tests or estimation schemes that work with local information. Graph stationarity was also studied in (Perraudin and Vandergheynst 2017) where the requirement of having a covariance matrix diagonalizable by the eigenvectors of the Laplacian GSO is adopted as a definition. This condition is shown to be equivalent to statistical invariance with respect to the translation operator introduced in (Shuman, Ricaud, and Vandergheynst 2016). When the shift \\({\\bf S}\\) coincides with the Laplacian of the graph and the eigenvalues of \\({\\bf S}\\) are all distinct, Definitions 12.1 and 12.2 are equivalent to those in Perraudin and Vandergheynst (2017). Hence, the definitions presented here differ from (Perraudin and Vandergheynst 2017) in that we consider general normal shifts instead of Laplacians and that we see Definition 12.1 as a definition, not a property. These are mathematically minor differences that are important in practice though; see Segarra et al. (2017) for more details.\n\nPerraudin, Nathanaël, and Pierre Vandergheynst. 2017. “Stationary Signal Processing on Graphs.” IEEE Transactions on Signal Processing 65 (13): 3462–77.\n\nShuman, David I, Benjamin Ricaud, and Pierre Vandergheynst. 2016. “Vertex-Frequency Analysis on Graphs.” Applied and Computational Harmonic Analysis 40 (2): 260–91.\n\nSegarra, Santiago, Antonio G Marques, Gonzalo Mateos, and Alejandro Ribeiro. 2017. “Network Topology Inference from Spectral Templates.” IEEE Transactions on Signal and Information Processing over Networks 3 (3): 467–83."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-02-GSO.html",
    "href": "posts/2_Studies/GRAPH/2023-07-02-GSO.html",
    "title": "Graph Shift Operator",
    "section": "",
    "text": "Definition(Djuric and Richard 2018): Given a normal shift operator \\({\\bf S}\\), we say that a graph signal \\({\\bf y}\\) is weakly stationary with respect to \\({\\bf S}\\) if, for all \\(a\\), \\(b\\), and \\(c \\leq b\\), the following equality holds:\nDjuric, Petar, and Cédric Richard. 2018. Cooperative and Graph Signal Processing: Principles and Applications. Academic Press.\n\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf y}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf y}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf y}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf y} \\Big)^H \\bigg].\\]\n\n\n\n\n\n\nNote\n\n\n\nUsing \\({\\bf S}\\) as the periodic shift operator \\({\\bf S}=\\begin{cases} 1 & j-j = 1 \\\\ 0 & o.w.\\end{cases}.\\), the definition is equivalent to the traditional stationarity definition in time series analysis.\n\n\n\nConjugate\n\\(E(y)=0\\)을 가정하고, \\(y = (y_1,y_2)\\)의 벡터를 가정했을 때,\n\\(Cov(y) = \\begin{pmatrix} cov(y_1,y_2) & cov(y_1,y_2) \\\\ cov(y_2,y_1) & cov(y_2,y_2) \\end{pmatrix}\\)\n\n\\(cov(y_1,y_1) = V(y_1) = E(y_1 - \\mu_1)^2 = E(y_1)^2 (\\therefore \\mu = 0)\\)\n\\(cov(y_2,y_1) = E(y_2 - \\mu_2)E(y_1 - \\mu_1) = E(y_2-y_1)\\)\n\\(cov(y_2,y_2) = V(y_2) = E(y_2 - \\mu_2)^2 = E(y_2)^2 (\\therefore \\mu = 0)\\)\n\\(cov(y_1,y_2) = E(y_1 - \\mu_1)E(y_2 - \\mu_2) = E(y_1-y_2)\\)\n\n\\(= E \\begin{pmatrix} y_1^2 & y_1y_2 \\\\ y_2 y_1 & y_2^2 \\end{pmatrix} = E(y y^\\top)\\)\n\\(y = (y_q y_2)^\\top\\)\n\\(y^\\top = (y_1,y_2)\\)\n\\(y y^\\top = \\begin{bmatrix} y1 \\\\ y_2 \\end{bmatrix} \\begin{bmatrix} y2 & y_2 \\end{bmatrix} = \\begin{bmatrix} y_1^2 & y_1y_2 \\\\ y_2y_1 & y_2^2 \\end{bmatrix}\\)\n\\(cov(y) = E(y t^\\top) = E(y y^H)\\) -> 확률변수가 복소수일 경우 가정 가능하다\n\\(cov(y\\text{의 } a \\text{만큼 평행이동}) = cov(y\\text{의 } b \\text{만큼 평행이동})\\)\n\\(cov((S^ay)(S^b y)^\\top) = cov((S^c y)(S^d y)^\\top)\\)\n\n결국, normal GSO가 주어질 때, \\(y\\)는 약정상성을 S에 대해 가지고 있다는 말이 된다.\n정상성 조건: 평균, 분산이 일정할때, 자기 공분산이 시차 t에만 의존할 때"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Kronecker product",
    "text": "Kronecker product\n크로네커곱의 정의는 아래와 같다.\n\\[{\\bf A} \\otimes {\\bf B}\n=\\begin{bmatrix}\na_{11}{\\bf B} & a_{12}{\\bf B} & \\dots & a_{1m}{\\bf B} \\\\\na_{21}{\\bf B} & a_{22}{\\bf B} & \\dots & a_{2m}{\\bf B} \\\\\n\\dots & \\dots & \\dots & \\dots \\\\\na_{n1}{\\bf B} & a_{n2}{\\bf B} & \\dots & a_{nm}{\\bf B} \\\\\n\\end{bmatrix}\\]\n두 행렬 \\({\\bf A}_{m\\times n}\\), \\({\\bf B}_{p\\times q}\\)의 크로네커곱 \\({\\bf A}\\otimes {\\bf B}\\)의 차원은 \\(mp \\times nq\\) 가 된다. 계산예시는 아래와 같다.\n\n\n\n위키에서 긁은 예제, 글씨가 좀 작음\n\n\n크로네커곱에 대한 성질들이 위키에 많이 있으니 참고하면 좋다.\n(예제1)\n\nA= [1 2\n    3 4]\nB= [0 5\n    6 7]\nC = kron(A, B)\n\n4×4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n(예제2)\n\nA= [1 -4 7; -2 3 3]\nB= [8 -9 -6 -5; 1 -3 -4 7; 2 8 -8 -3; 1 2 -5 -1]\nC = kron(A, B)\n\n8×12 Matrix{Int64}:\n   8   -9  -6   -5  -32   36   24   20  56  -63  -42  -35\n   1   -3  -4    7   -4   12   16  -28   7  -21  -28   49\n   2    8  -8   -3   -8  -32   32   12  14   56  -56  -21\n   1    2  -5   -1   -4   -8   20    4   7   14  -35   -7\n -16   18  12   10   24  -27  -18  -15  24  -27  -18  -15\n  -2    6   8  -14    3   -9  -12   21   3   -9  -12   21\n  -4  -16  16    6    6   24  -24   -9   6   24  -24   -9\n  -2   -4  10    2    3    6  -15   -3   3    6  -15   -3"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Khatri–Rao product",
    "text": "Khatri–Rao product\n카트리-라오곱은 매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 같은 차원의 블락매트릭스로 정의될때 각 서브매트릭스의 크로네커 곱으로 정의된다. 정의와 계산예시는 아래와 같다.\n\n\n\n예시1: 위키에서 긁은 그림\n\n\n또 다른 계산예시는 아래와 같다. 이 예제는 중요하니까 구현해보자.\n\n\n\n예시2: 위키에서 긁은 그림\n\n\n(예제1)\n\nC= [1 2 3 \n    4 5 6 \n    7 8 9] \nD= [1 4 7\n    2 5 8\n    3 6 9]\n\n3×3 Matrix{Int64}:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:3]...)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81\n\n\n이건 자주 쓸일이 있을것 같으니까 함수로 저장하자.\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#181 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프 표현",
    "text": "그래프 표현\n아래의 그림을 살펴보자.\n\n\n\n그래프의 개념을 이해하는 필요한 그림, 일단 오른쪽의 \\({\\bf S}\\)는 무시할 것\n\n\n오른쪽의 \\({\\bf S}\\)는 무시하고 왼쪽의 그래프만 살펴보자. 이 그림에는 6개의 노드가 있고 각각의 노드는 저 마다의 연결구조를 가진다. 이러한 연결구조는 \\({\\bf G}=({\\bf N},{\\bf E})\\) 으로 표현할 수 있는데 여기에서 \\({\\bf N}\\)은 노드들의 집합이고 \\({\\bf E}\\)는 엣지들의 집합이다.1 보통 \\({\\cal E}\\)는 복잡하므로 연결정보를 매트릭스 \\({\\bf E}\\)로 표현하는데 이러한 \\({\\bf E}\\)를 인접행렬이라고 부른다. 인접행렬의 각 원소는 \\(E_{ij}= \\begin{cases} 1 & (i,j) \\in {\\cal E} \\\\ 0 & o.w \\end{cases}\\) 와 같이 정의한다. 이 그림의 경우 \\({\\cal N}\\) 와 \\({\\cal E}\\), \\({\\bf E}\\) 는 아래와 같다.1 노드 \\(i\\)에서 노드 \\(j\\)로 향하는 연결이 있다면 \\((i,j) \\in {\\cal E}\\)이다.\n\n\\({\\cal N}=\\{1,2,3,4,5,6\\}\\)\n\\({\\bf E}=\\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 \\\\ 1 & 0 & 1 & 0 & 1 & 0\\\\ 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\({\\cal E} = \\{(i,j) : E_{ij}=1 \\}\\)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "GSO",
    "text": "GSO\n후에 자세히 서술하겠지만 전통적인 시계열분석기법을 그래프신호로 확장하기 위해서는 단지 퓨리에변환 대신에 그래프퓨리에 변환을 사용하면 된다. 즉 퓨리에변환을 일반화한 그래프퓨리에변환을 잘 정의하면 된다.\n전통적인 신호처리 영역에서의 퓨리에변환은 시계열자료의 인접성을 의미하는 행렬 \\({\\bf B}\\)2의 고유행렬의 켤레전치로 정의할 수 있다. 이를 이용하면 그래프 퓨리에변환은 그래프자료의 인접성을 의미하는 행렬3의 고유행렬의 켤레전치로 정의할 수 있음을 유추할 수 있다. 즉 비유클리드 자료에서도 \\({\\bf B}\\)에 대응하는 어떠한 매트릭스가 정의되어야 하는데 (그리고 이 매트릭스는 그래프자료의 인접성에 대한 정보가 있어야 한다) 이 매트릭스를 \\({\\bf S}\\)라고 정의하고 grahp shift operator (GSO) 라고 이름 붙인다.2 원래는 평행이동을 의미하는 행렬이지만, 이걸 인접성을 의미하는 행렬로 해석할 수도 있다. 어차피 인접한 곳으로 이동할 수 있으니까..3 예를들면 인접행렬 \\({\\bf E}\\)와 같은 행렬\n주어진 그래프 \\({\\cal G}=({\\cal N},{\\cal E})\\) 에 대하여 GSO \\({\\bf S}\\)는 \\({\\bf E}+{\\bf I}\\)의 값이 1인 영역에만 값이 있는 어떠한 행렬이다. 다시 아래의 그림을 생각하여 보자.\n\n\n\nGSO의 개념을 이해하는데 필요한 그림\n\n\n왼쪽그래프의 GSO는 오른쪽과 같은 행렬 \\({\\bf S}\\)가 된다. 이제 \\({\\bf S}\\) 의 고유벡터행렬을 구한 뒤에 그것의 켤레전치를 \\({\\bf GFT}\\) 행렬로 정의하면 될 것 같다. 문제는 “\\({\\bf S}\\)의 고유벡터행렬이 항상 존재하는가?” 인데, 사실 이게 항상 존재한다는 보장이 없다. 즉 \\({\\bf S}\\)의 고유벡터 행렬이 존재 안할 수도 있다. 따라서 GSO \\({\\bf S}\\)가 고유분해가능하다는 조건이 추가적으로 필요한데 이러한 조건을 만족하는 GSO를 normal GSO라고 부른다. 우리는 당연히 normal GSO에 대해서만 관심이 있으므로 앞으로 특별한 언급이 없는한 GSO는 모두 normal GSO라고 가정한다."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Periodogram, correlogram, and LS estimator",
    "text": "Periodogram, correlogram, and LS estimator\nFrom \\({\\bf C}_{\\tilde{\\bf x}}:= \\mathbb{E}\\left[\\tilde{\\bf x}\\tilde{\\bf x}^H \\right]=\\mathbb{E}\\left[({\\bf V}^H{\\bf x})({\\bf V}^H{\\bf x})^H \\right]=\\text{diag}({\\bf p})\\) it follows that one may express the PSD as \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\). That is, the PSD is given by the expected value of the squared frequency components of the random process. This leads to a natural approach for the estimation of \\({\\bf p}\\) from a finite set of \\(R\\) realizations of the process \\({\\bf x}\\). Indeed, we compute the \\({\\bf GFT} \\tilde{\\bf x}_r = {\\bf V}^H{\\bf x}_r\\) of each observed signal \\({\\bf x}_r\\) and estimate \\({\\bf p}\\) as\n\\[\n\\hat{\\bf p}_{pg}:= \\frac{1}{R}\\sum_{r=1}^R|\\tilde{\\bf x}_r|^2=\\frac{1}{R}\\sum_{r=1}^{R}|{\\bf V}^H{\\bf x}_{r}|^2.\n\\]\nThe estimator \\(\\hat{\\bf p}_{pg}\\) is termed periodogram due to its evident similarity with its homonym5 in classical estimation. It is simple to show that \\({\\bf p}_{pg}\\) is an unbiased estimator, that is, \\(\\mathbb{E}[\\hat{\\bf p}_{pg}]= {\\bf p}\\). A more detailed analysis of the performance of \\(\\hat{\\bf p}_{pg}\\), for the case where the observations are Gaussian, is given in Proposition 12.1.65 동음이의어6 Proposition 12.1은 뒤에 다루는데 \\(\\hat{\\bf p}_{pg}\\)의 분산에 대한 서술이 있음. 분산은 \\(\\mathbb{V}[\\hat{\\bf p}_{pg}]=\\frac{2}{R}\\text{diag}^2({\\bf p})\\)와 같음\nAn alternative nonparametric estimation scheme, denominated correlogram, can be devised by starting from the definition of \\({\\bf p}\\) in\n\\[{\\bf p}:=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big).\\]\nNamely, one may substitute \\({\\bf C}_{\\bf x}\\) in above equation by the sample covariance \\(\\hat{\\bf C}_{\\bf x} = \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\) computed based on the available observations to obtain\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\nNotice that the matrix \\({\\bf V}^H\\hat{\\bf C}_{\\bf x}{\\bf V}\\) is in general, not diagonal because the eigenbasis of \\(\\hat{\\bf C}_{\\bf x}\\) differs from \\({\\bf V}\\), the eigenbasis of \\({\\bf C}_{\\bf x}\\). Nonetheless, we keep only the diagonal elements \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x}{\\bf v}_i\\) for \\(i = 1, \\dots , N\\) as our PSD estimator. It can be shown that the correlogram \\({\\bf p}_{cg}\\) and the periodogram \\({\\bf p}_{pg}\\) lead to identical estimators, as is the case in classical signal processing.\nThe correlogram can also be interpreted as an LS estimator. The decomposition in \\({\\bf C}_{\\bf x}={\\bf V}\\text{diag}({\\bf p}){\\bf V}^H\\) allows a linear parameterization of the covariance matrix \\({\\bf C}_{\\bf x}\\) as\n\\[\n{\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H.\n\\]\nThis linear parametrization will also be useful for the sampling schemes developed in Section 12.4. Vectorizing \\({\\bf C}_{\\bf x}\\) in \\({\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H\\) results in a set of \\(N^2\\) equations in \\({\\bf p}\\)\n\\[\n{\\bf c}_{\\bf x} = \\text{vec}({\\bf C}_{\\bf x})=\\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf G}_{np}{\\bf p},\n\\]\nwhere \\(\\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf v}_i^\\ast \\otimes {\\bf v}_i\\). Relying on the Khatri-Rao product, we then form the \\(N^2 \\times N\\) matrix \\({\\bf G}_{np}\\) as\n\\[\n{\\bf G}_{np}:= \\left[{\\bf v}_1^\\ast \\otimes {\\bf v}_1, \\dots, {\\bf v}_N^\\ast \\otimes {\\bf v}_N \\right] = {\\bf V}^\\ast \\odot {\\bf V}.\n\\]\n\nHere \\(\\otimes\\) denote the Kronecker matrix product and \\(\\odot\\) denote the Khatri-Rao matrix product.\n\nUsing the sample covariance matrix \\(\\hat{\\bf C}_{\\bf x}\\) as an estimate of \\({\\bf C}_{\\bf x}\\), we can match the estimated covariance vector \\(\\hat{\\bf c}_{\\bf x}=\\text{vec}(\\hat{\\bf C}_{\\bf x})\\) to the true covariance vector \\({\\bf c}_{\\bf x}\\) in the LS sense as\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\nIn other words, the LS estimator minimizes the squared error \\(\\text{tr}\\left[\\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)^T \\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)\\right]\\). From expression \\(\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\) it can be shown that the \\(i\\)th element of \\(\\hat{\\bf p}_{ls}\\) is \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x} {\\bf v}_i\\). Combining this with Eq.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right]\\]\nwe get that the LS estimator \\(\\hat{\\bf p}_{ls}\\) and the correlogram \\(\\hat{\\bf p}_{cg}\\) —and hence the periodogram as well— are all identical estimators. The estimators derived in this subsection do not assume any data distribution and are well suited for cases where the data probability density function is not available. In what follows, we provide performance bounds for these estimators under the condition that the observed signals are Gaussian."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD",
    "text": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD\n\n전통적인 분석방법\n클래식한 정상시계열은 유한차수의 ARMA로 근사할 수 있음이 알려져 있다7. 유한차수의 ARMA의 계수 \\(p\\),\\(q\\)를 적절하게 추정하기 위해서는 시계열 \\({\\bf x}\\)를 SACF plot 혹은 SPACF plot 을 이용하면 된다. 이때 SACF 혹은 SPACF 의 그림을 살펴보고 적절한 모형을 선택하기 위해서는 유한차수 ARMA의 이론적 ACF의 모양을 알면 되는데,8 이를 바꾸어서 말하면 결국 정상시계열 \\({\\bf x}\\)의 모든 정보는 ACF에 들어있다는 의미가 된다. 즉 정상시계열은 ACF만 잘 추정하면 모든 것이 해결된다.7 Wold’s theorem8 예를들어 “coef가 0.9인 AR(1)의 경우 lag=1 에 대한 이론적 ACF값이 0.9, lag=2에 대한 ACF값이 0.81, … 와 같이 되더라~” 하는식의\n그런데 ACF의 모든 정보는 다시 아래의 행렬에 들어있다.\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^T]\\]\n여기에서 \\({\\bf x}\\)는 realization이 아니라 확률벡터를 의미함을 유의하자.9 따라서 정상시계열의 경우 \\({\\bf C}_{\\bf x}\\)를 잘 추정하면 모든것이 해결된다고 생각하면 된다.9 보통 수리통계에서는 확률변수를 \\(X\\) realization을 \\(x\\)로 표현하지만 여기에서는 매트릭스를 대문자로 쓰고 있어서 그런식으로 표현하기 어렵다, 그래서 그때 그때 이것이 확률변수인지 realization인지 따져봐야 한다\n\n참고: 정상시계열의 경우 ACF 만 정확하게 알아도 (반대로 PACF만 정확하게 알아도) 이론상 모든 모형을 특정할 수 있다. 즉 정상시계열의 모형을 특정하기 위해서는 ACF plot, PACF plot 하나만 있어도 충분하다. (Wold’s Thm은 떠올리면 모든 정상시계열은 무한MA로 유니크하게 표현할 수 있는데, 이는 PACF plot을 가지고 모든 정상시계열을 유니크하게 특정할 수 있다는 것을 의미한다) 다만 좀 더 모형을 특정하는 과정을 용이하게 하기 위해서 실전에서는 SACF plot 과 SPACF plot 을 함께 보는 것이 유리하다.\n\n(예제) AR(1) 모형\n왜 ACF의 모든정보를 \\({\\bf C}_{\\bf x}\\)로 부터 알수 있는지 코드를 통하여 실습하여 보자. (바로 이해된다면 사실 이 예제는 스킵해도 무방함) 아래와 같은 모형을 가정하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n여기에서 \\(\\epsilon_t\\)는 서로 독립인 표준정규분포를 따른다. 이 모형에서 길이가 100인 시계열을 임의로 발생시키자.\n\nx = zeros(100*1000)\nx[1] = randn()\nfor t in 2:100\n    x[t] = 0.5*x[t-1] + randn()\nend\n\n모형에서 생성된 하나의 시계열을 시각화 하면 아래와 같다.\n\nplot(x) # 그냥 그려본것임. 별 의미는 없음\n\n\n\n\nlag=1일 경우 이 시계열의 SACF를 계산하면 아래와 같다.\n\nx[1:99] .* x[2:100]\n\n99-element Vector{Float64}:\n  1.587897526021493\n  1.130306190921068\n  0.5698214432110668\n  0.4648189302568683\n  0.3099446153360606\n  0.36362604534744775\n  0.8191871414624922\n -0.1720390842292145\n -0.06301214708310766\n  0.026414715508855904\n -0.007988283356933327\n -0.04178812545299474\n  0.22453267567940685\n  ⋮\n  3.931333581073927\n  1.315564948810858\n  0.9096080102581454\n  0.5410986320348997\n  0.29627801400693676\n  1.0673283524686212\n -1.0394649044573636\n  2.80195248208142\n  4.152973765526384\n  2.316315764368524\n  0.978758337765867\n -0.5840281943972468\n\n\n\n이 계산결과는 각 \\(t\\)에 대하여 \\(x_{t-1}x_t\\) 를 계산한 것과 같다.\n\n이 수열들의 평균은 아래와 같다.\n\nx[1:99] .* x[2:100] |> mean\n\n0.5835563885014224\n\n\n\n이 계산결과는 \\(\\frac{1}{99}\\sum_{t=2}^{100} x_{t-1}x_t\\)를 계산한 것과 같다.\n\n이론적인 값인 0.5 근처의 값이 잘 나옴을 알 수 있다.\nlag=2일 경우도 마찬가지로 구할 수 있다.\n\nx[1:98] .* x[3:100] |> mean\n\n0.38420263596668275\n\n\n이러한 숫자들은 그런데 \\({\\bf x}{\\bf x}^T\\)를 이용하여서도 구할 수 있다.1010 참고로 여기에서 \\({\\bf x}\\)는 확률벡터가 아니라 realization을 의미함\n\nx*x'\n\n100×100 Matrix{Float64}:\n  0.760108    1.5879      0.541064   …  -1.57394    -0.472676    0.939172\n  1.5879      3.31719     1.13031       -3.28802    -0.987441    1.96197\n  0.541064    1.13031     0.385143      -1.12037    -0.336463    0.668527\n  0.800507    1.67229     0.569821      -1.65759    -0.497799    0.989089\n  0.441361    0.922022    0.314172      -0.913915   -0.274462    0.545336\n  0.533784    1.1151      0.379961   …  -1.10529    -0.331936    0.659531\n  0.517803    1.08171     0.368586      -1.0722     -0.321998    0.639786\n  1.20252     2.51212     0.855987      -2.49003    -0.747794    1.48581\n -0.108745   -0.227173   -0.0774074      0.225175    0.0676234  -0.134363\n  0.440444    0.920106    0.313519      -0.912016   -0.273892    0.544203\n  0.0455859   0.0952309   0.0324492  …  -0.0943935  -0.0283478   0.0563249\n -0.133198   -0.278257   -0.0948139      0.27581     0.0828298  -0.164577\n  0.238468    0.498169    0.169748      -0.493789   -0.148292    0.294646\n  ⋮                                  ⋱                          \n  2.04697     4.2762      1.45708       -4.2386     -1.27291     2.52919\n  0.488514    1.02053     0.347736      -1.01155    -0.303784    0.603596\n  1.41531     2.95665     1.00746    …  -2.93065    -0.880119    1.74873\n  0.290602    0.60708     0.206858      -0.601742   -0.180712    0.359062\n  0.774954    1.61891     0.551632      -1.60468    -0.481908    0.957516\n  1.04688     2.18698     0.745197      -2.16775    -0.651007    1.2935\n -0.754723   -1.57665    -0.537231       1.56279     0.469328   -0.932519\n -2.82194    -5.89516    -2.00873    …   5.84333     1.75484    -3.48673\n -1.11863    -2.33686    -0.796269       2.31632     0.695624   -1.38215\n -1.57394    -3.28802    -1.12037        3.25911     0.978758   -1.94472\n -0.472676   -0.987441   -0.336463       0.978758    0.293936   -0.584028\n  0.939172    1.96197     0.668527      -1.94472    -0.584028    1.16042\n\n\n여기에서 각 원소들이 의미하는 바는 아래와 같다.\n\n대각선의 원소: \\(x_t^2,~ t=1,2,\\dots,100\\) 을 의미\n대각선 한칸 위, 혹은 한칸 아래: \\(x_{t-1} x_t~ t=2,3,\\dots,100\\) 을 의미\n대각선 두칸 위, 혹은 두칸 아래: \\(x_{t-2} x_t~ t=3,4,\\dots,100\\) 을 의미\n\n\n\n\nx*x'의 계산결과를 캡쳐한 그림, 이것은 \\(\\hat{\\bf C}_{\\bf x}\\)를 의미함\n\n\n확인해보자.\nlag=1, 스크린샷의 노란색\n\n(x[1:99] .* x[2:100])[1:5]\n\n5-element Vector{Float64}:\n 1.587897526021493\n 1.130306190921068\n 0.5698214432110668\n 0.4648189302568683\n 0.3099446153360606\n\n\n\nlag1에 해당하는 숫자들임. 이는 스크린샷에서 노란색으로 표현된 1.589, 1.13031, 0.569821 … 등과 일치한다.\n\nlag=2, 스크린샷의 빨간색\n\n(x[1:98] .* x[3:100])[1:5]\n\n5-element Vector{Float64}:\n 0.5410642277088621\n 1.6722932576420804\n 0.3141719983177106\n 0.5621541352252872\n 0.30066534927151267\n\n\n\nlag2에 해당하는 숫자들임. 이는 스크린샷에서 빨간색으로 표현된 숫자들인 0.54164, 1.67229, 0.31417 … 등과 일치한다.\n\n\n\n스펙트럼 방법\n지금까지는 정상시계열일 경우 ACF를 이용한 간단한 분석방법을 다시 복습했다. 그리고 \\({\\bf C}_{\\bf x}\\)가 ACF를 구함에 필요한 모든정보를 가지고 있음을 이해했다. 한편 \\({\\bf C}_{\\bf x}\\)은 positive definite matrix 이므로 아래와 같이 분해가능하다.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식표현을 잘 해석하면 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf V}\\)와 \\({\\bf p}\\)에 담겨있다는 사실을 이해할 수 있다. 그런데 정상시계열일 경우 한정하여 \\({\\bf C}_{x}\\)의 고유벡터행렬은 \\({\\bf B}\\)의 고유벡터행렬과 일치한다는 사실을 알고 있다. 따라서 \\({\\bf V}\\)는 \\({\\bf B}\\)로 부터 그냥 알 수 있는 정보이다. 따라서 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf p}\\)에 담겨있다는 사실을 알 수 있다. 이는 적절한 \\({\\bf p}\\)를 추정하는 일은 적절한 \\({\\bf C}_{\\bf x}\\)를 추정하는 것과 같다는 사실을 알려준다.\n요약하면 아래와 같다.\n\n임의의 정상시계열은 이론적인 ACF (혹은 PACF)를 잘 추정하면 유니크하게 특정할 수 있다. (Wold’s Thm)\nACF를 잘 추정한다는 말은 \\({\\bf C}_{\\bf x}\\)를 잘 추정한다는 의미이다.\n그런데 \\({\\bf p}\\)를 잘 추정하면 \\({\\bf C}_{\\bf x}\\)를 잘 추정하는 일이 된다.\n따라서 임의의 정상시계열은 \\({\\bf p}\\)를 잘 추정하면 유니크하게 특정할 수 있다는 결론을 얻는다.\n\n여기에서 \\({\\bf p}\\)를 power spectral density 라고 부른다. 일반적으로 정상시계열을 분석하기 위해서는 \\({\\bf C}_{\\bf x}\\)를 특정하거나, \\({\\bf p}\\)를 특정하면 되는데 여기에서 \\({\\bf p}\\)를 특정한뒤 \\({\\bf p}\\)로 부터 \\({\\bf C}\\)를 역으로 해석하는 방법론을 spectral analysis라고 부른다. 경우에 따라서 \\({\\bf C}_{\\bf x}\\)를 특정하는 것이 용이할 수도 있지만 \\({\\bf p}\\)를 특정하고 해석하는 것이 용이할 때도 있다.\n그렇다면 주어진 시계열 \\({\\bf x}\\)에 대하여 \\({\\bf p}\\)를 어떻게 구할까? 직관적으로 생각하면 단순히 아래의 알고리즘으로 구하면 된다는 것을 알 수 있다.\n\n\\({\\bf C}_{\\bf x}\\)를 알아낸다.\n\\({\\bf C}_{\\bf x}\\)를 고유분해하여 \\({\\bf p}\\)를 구한다.\n\n또 다른 방법으로는 교재에 소개된 바 있는 아래의 수식을 이용하는 것이다.1111 이 수식이 성립하는 이유는 조금 손으로 써보면 금방 알 수 있음\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\]\n이것을 이용하면 아래와 같은 알고리즘을 떠올릴 수 있다.\n\n\\({\\bf B}\\)의 고유벡터행렬 \\({\\bf V}\\)를 구하고 \\({\\bf V}^H{\\bf x}\\)를 계산한다.\n계산된 결과를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n그런데 \\({\\bf V}^H{\\bf x}= {\\bf DFT} \\cdot {\\bf x}\\) 이므로 1의 과정을 아래와 같이 바꾸어 서술할 수 있다.\n\n\\({\\bf x}\\)를 퓨리에변환하여 \\(\\tilde{\\bf x} = {\\bf DFT} \\cdot {\\bf x}\\) 를 계산한다.\n\\(\\tilde{\\bf x}\\)를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n즉 임의의 시계열을 퓨리에변환한 뒤 제곱하면 \\({\\bf p}\\)를 얻을 수 있다.\n(예제2) – 하나의 realization에서 \\(\\hat{\\bf p}\\)를 구해보자.\n(예제1에 이어서) 아래의 모형에서 생성된 \\({\\bf x}\\)를 다시 고려하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n\nplot(x)\n\n\n\n\n이 자료의 PSD \\({\\bf p}\\)는 아래와 같이 구할 수 있다.\n단계1: \\({\\bf x}\\)의 DFT를 계산\n\nx̃ = fft(x) \n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\\({\\bf B}\\)를 설정하고 고유값분해 하기 귀찮아서 그냥 DFT해주는 패키지 사용함\n\n단계2: \\(\\hat{\\bf p}\\)를 계산\n\np̂ = abs.(x̃).^2\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n참고\nfft(x) 대신에 아래의 코드를 이용해도 된다.\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\nV'x\n\n100-element Vector{ComplexF64}:\n  -5.756917285643587 + 0.0im\n -19.082672090492103 - 1.0178306444775291im\n   14.23050682476898 - 11.867854578090007im\n  3.8980118254428824 + 1.2603018602424476im\n  -16.15797305318818 + 27.48824632227092im\n   12.32574209329044 - 1.5134316695905325im\n  3.9542122497256385 + 15.369129638224617im\n    9.51693811050782 + 19.371467179753516im\n  -19.38292930624826 + 9.495062886234233im\n -7.8539348514784155 + 4.134711886071595im\n -14.072349901900417 - 5.945064076174276im\n -14.596266922162371 + 3.447776409279244im\n   5.857720447482956 + 5.7388951128385735im\n                     ⋮\n   5.857720447482839 - 5.738895112838781im\n -14.596266922162307 - 3.4477764092792627im\n  -14.07234990190023 + 5.945064076174198im\n  -7.853934851478599 - 4.134711886071242im\n -19.382929306248577 - 9.49506288623372im\n   9.516938110507212 - 19.371467179753736im\n  3.9542122497250025 - 15.369129638224603im\n  12.325742093290597 + 1.5134316695903638im\n  -16.15797305318867 - 27.488246322270854im\n  3.8980118254424903 - 1.2603018602428118im\n  14.230506824769146 + 11.867854578089572im\n   -19.0826720904922 + 1.0178306444775123im\n\n\n진짜 똑같은지 확인\n\nfft(x)\n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\n전통적인 방법과 스펙트럼 방법의 비교\n시계열자료의 전통적인 분석과 spectral analysis는 대충 아래의 과정으로 비교 설명할 수 있다.\n\n\n\n\n\n\n\n\n단계\n전통적인 방법\n스펙트럴 분석\n\n\n\n\n1\n\\({\\bf x}\\)의 plot을 그려봄\n\\({\\bf x}\\)의 plot을 그려봄\n\n\n2\nSACF plot, SPACF plot 을 그려봄\nPSD plot을 그려봄\n\n\n3\nACF를 추정 (=ARMA(\\(p\\),\\(q\\))에 대응하는 파라메터를 추정)\n\\({\\bf p}\\)를 추정\n\n\n4\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n\n\n\n눈여겨 볼 점은 PSD plot의 존재이다. 전통적인 시계열에서 SACF plot 과 비슷하게 스펙트럼 방법에서 시계열을 분석하기 위해 필요한 매우 중요한 시각화 이다. 간단하게 비교를 하면 아래와 같다.\nSACF plot\n\nx축: lag=0, lag=1, ….\ny축: lag에 대응하는 상관계수값\n\nPSD plot\n\nx축: \\(\\Omega=\\big\\{\\frac{k}{N}:~ \\text{for}~ k=0,\\dots, N-1\\big\\}\\), 정규화된 freq를 의미함\ny축: 주파수에 대응하는 power값\n\n전통적인 방법에 비하여 스펙트럴 분석이 가지는 장점은 위의 표에서 소개한 일반적인 분석루틴이 시계열이 아닌 그래프신호로 쉽게 확장가능 하다는 점이다12. 따라서 앞으로는 전통적인 시계열 분석방법 대신 스펙트럴 분석만을 다룰 것이다. 스펙트럴 분석의 핵심적인 부분은 \\({\\bf p}\\)를 추정하는 방법과 추정량의 점근적 성질들을 파악하는 것이다. 이 포스트에서는 \\({\\bf p}\\)를 추정하는 방법만을 다룬다.12 퓨리에 변환대신에 그래프 퓨리에 변환을 이용하기만 하면된다"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프신호에서의 PSD의 추정",
    "text": "그래프신호에서의 PSD의 추정\n이제 그래프 신호에서 \\({\\bf p}\\)를 추정하는 방법에 대하여 살펴보자. 그래프이동변환 (Graph Shift Operator, GSO)13 \\({\\bf S}={\\bf V}{\\bf \\Lambda}{\\bf V}^H\\)에 대하여 정상인 시계열 \\({\\bf x}\\)를 고려한다. 이 신호의 그래프퓨리에 변환14은 아래와 같이 구할 수 있다.13 Back shift operator의 일반화 버전14 좀 더 정확하게는 \\({\\bf V}^H\\) 에 대한 그래프 변환이라고 한다\n\\[\\tilde{\\bf x}={\\bf GFT} {\\bf x} = {\\bf V}^H{\\bf x}\\]\n여기에서 \\(\\tilde{\\bf x}\\)를 \\({\\bf x}\\)의 주파수응답(frequency representation)이라고 부른다.15 우리는 아래의 수식에서 \\({\\bf p}\\)의 값에 관심이 있다.15 이 \\(\\tilde{\\bf x}\\)를 그냥 graph Fourier transform이라고 부르는 사람도 많다. 즉 그래프퓨리에변환이 (1) 변환매트릭스 \\({\\bf GFT}\\)자체를 지칭할때도 있고 (2) 트랜스폼된 결과 \\(\\tilde{\\bf x}\\)를 지칭할때도 있음. 교재에서는 변환은 graph Fourier transform, 그리고 변환된 결과는 \\({\\bf x}\\)의 주파수응답이라고 한다.\n\\[{\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\]\n여기에서 \\({\\bf p}\\)를 PSD (power spectrum density) 라고 한다. \\({\\bf p}\\)가 포함된 표현식은 위의 수식 이외에도 2개가 더 있다. 이를 모두 요약하면 아래와 같다1616 약간의 계산을 통하면 1,2,3이 쉽게 같은 수식임을 알 수 있음\n\n\\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)17\n\\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n\\({\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\)\n\n17 이 수식을 살짝 정리하면 \\({\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\) 와 같이 보다 예쁜 수식을 얻을 수 있음위의 표현중 3.에서 \\({\\bf c}_{\\bf x}\\)은 \\({\\bf C}_x\\)를 벡터화한 것이며 \\({\\bf G}_{np}\\)는 \\({\\bf V}^\\ast\\) 와 \\({\\bf V}\\)를 열별-크로네커곱 (column-wise Kronecker product) 이다. 이때 \\({\\bf G}_{np}\\)의 정의가 조금 생소하니 한번 계산하여 보자.\n(예제) 아래와 같은 GSO \\({\\bf B}\\)를 고려하자.\n\nB= [0 1 0 0 \n    0 0 1 0 \n    0 0 0 1 \n    1 0 0 0]\n\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  1  0\n 0  0  0  1\n 1  0  0  0\n\n\n이러한 GSO에 대하여 \\({\\bf G}_{np}\\)는 아래와 같이 구할 수 있다.\n(1) \\({\\bf V}\\)를 정의\n\nV = [i*j for i in 0:3 for j in 0:3] |> \n    x -> reshape(x,(4,4)) .|> \n    x -> exp(im * (2π/4) * x) \n\n4×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n16×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n\n\n위에서 언급한 표현식 1,2,3 을 이용하면 \\({\\bf p}\\)를 추정하는 세 가지 방법을 각각 정의할 수 있다. 하나씩 살펴보자.\n\n1. \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 수식 \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)를 적당히 변형하면 아래를 얻을 수 있다.\n\\[{\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\]\n여기에서\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^H]\\approx \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_t{\\bf x}_r^H\\]\n이므로 이 수식에 근거하여 \\({\\bf p}\\)을 추정한다면 아래와 같이 할 수 있다.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면18, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.18 대부분은 관측한 시계열이 하나겠지..\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H{\\bf x}_r{\\bf x}_r^H{\\bf V} \\right].\\]\n\n주의: 여기에서 \\({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V}\\) 는 항상 대각행렬이지만 \\({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V}\\) 은 대각행렬이 아닐수도 있음을 유의하자. 즉 이론적인 모수는 대각행렬이지만 sample version은 대각행렬이 아닐 수 있다. 대각선이 아닌 원소는 버리면 된다.)\n\n\n아이디어: 혹시 대각선이 아닌 원소들을 이용하여 오차항 \\(\\epsilon_t\\)의 분산을 추정할 수도 있지 않을까? 이미 연구가 있겠지?\n\n(예제)\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n\np̂ = diag(V' * (x*x') * V)\n\n100-element Vector{ComplexF64}:\n 33.142096633741986 + 0.0im\n 365.18435333408354 + 1.5376069362644531e-13im\n  343.3532967764883 + 6.904176529646917e-14im\n 16.782856970223083 - 3.5538396658301444e-14im\n 1016.6837790613963 + 5.475049904926759e-15im\n 154.21439356883144 + 6.4512443306088e-14im\n  251.8459403524346 + 2.1316282072803006e-14im\n 465.82585169550384 + 1.816929057526117e-13im\n 465.85416770456044 + 4.1584439183295984e-14im\n    78.780135032089 + 1.3472456770553478e-14im\n 233.37481863133462 + 6.315728724701355e-14im\n 224.93817023139385 - 3.472109560086835e-14im\n  67.24780595702241 + 7.105427357601002e-14im\n                    ⋮\n   67.2478059570233 + 6.384723798533952e-14im\n 224.93817023139195 + 1.9727655769954595e-14im\n 233.37481863132837 - 2.1872689567834747e-14im\n    78.780135032089 + 1.917599080404094e-14im\n 465.85416770456294 + 4.808950231511622e-14im\n 465.82585169550094 - 4.890486289860305e-14im\n  251.8459403524291 + 2.0146681724568905e-14im\n  154.2143935688347 - 1.0948596967617507e-13im\n 1016.6837790614081 + 1.2114814701286432e-13im\n  16.78285697022108 + 2.376159104534641e-14im\n 343.35329677648286 + 1.1310381241837407e-14im\n 365.18435333408746 + 4.574214786667376e-14im\n\n\n\n\n2. \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\approx \\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n따라서 \\(\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2\\) 를 PSD \\({\\bf p}\\)에 대한 추정량이라고 생각할 수 있다. 이러한 추정량을 기호로 \\(\\hat{\\bf p}_{pg}\\)라고 정의하고 periodogram이라고 부른다. 즉\n\\[\\hat{\\bf p}_{pg}=\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n\\[\\hat{\\bf p}_{pg}=|{\\bf V}^H {\\bf x}_r|^2 \\]\n즉 이 경우 \\(\\hat{\\bf p}_{pg}\\)는 단순히 관측시계열 \\({\\bf x}_r\\)의 그래프 퓨리에 변환 \\(\\tilde{\\bf x}={\\bf V}^H{\\bf x}_r\\) 결과에 절대값을 취하고 제곱한 것과 같다.\n(예제)\n스펙트럼방법챕터 예제2에서 이미 보여준 적 있다. 주어진 시계열 \\({\\bf x}\\)에 대하여 \\(\\hat{\\bf p}_{pg}\\)를 구하는 방법을 요약하면 아래와 같다.\n\nx̃ = fft(x) # 단계1: GFT, 이 신호는 시계열이라서 GFT대신에 DFT를 써도 된다.\np̂ = abs.(x̃).^2 # 단계2: hat p\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n\n\n3. \\({\\bf c}_{\\bf x} = {\\bf G}_{np} {\\bf p}\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식으로부터 아래를 얻을 수 있다.\n\\[{\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\]\n여기에서 \\({\\bf c}_{\\bf x}\\) 대신에 \\(\\hat{\\bf c}_{\\bf x}\\) 를 대입하면 아래와 같이 생각할 수 있다.\n\\[\\hat{\\bf c}_{\\bf x} \\approx  {\\bf G}_{np} {\\bf p}\\]\n이 문제는 아래와 같은 회귀모형으로 생각할 수 있다.\n\n\n\n\n\n\n\n\n\n회귀모형\n우리의 문제\n\n\n\n\n모형\n\\({\\bf y} \\approx {\\bf X}{\\boldsymbol \\beta}\\)\n\\(\\hat{\\bf c}_{\\bf x} \\approx {\\bf G}_{np}{\\bf p}\\)\n\n\n설명변수\n\\({\\bf X}\\)19\n\\({\\bf G}_{np}\\)20\n\n\n반응변수\n\\({\\bf y}\\)21\n\\(\\hat{\\bf c}_{\\bf x}\\)22\n\n\n추정하고 싶은 파라메터\n\\({\\boldsymbol \\beta}\\)23\n\\(\\hat{\\bf p}\\)24\n\n\n오차항\n대부분 정규분포를 가정\n??? 모르겠는데??\n\n\n\n19 (n,p) matrix20 (N²,N) matrix21 (n,1) col-vector22 (N²,1) col-vector23 (p,1) col-vector24 (N,1) col-vector회귀분석에서 아래의 수식이 익숙하다면\n\\[\n\\hat{\\boldsymbol \\beta}_{ls} = \\underset{\\boldsymbol \\beta}{\\operatorname{argmin}} \\|{\\bf y}-{\\bf X}{\\boldsymbol \\beta}\\|_2^2=({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}.\n\\]\n\\({\\bf p}\\)를 추정하기 위한 아래의 수식도 쉽게 이해할 수 있다. (의문: 그런데 왜 MSE를 손실함수로 쓰고 있는 거야? 오차항이 설마 정규분포?)\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\n(예제)\n(1) \\({\\bf V}\\)를 정의\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n10000×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im      0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im   …  0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im      0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im    …  0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n 1.0+0.0im       1.0+0.0im                1.0+0.0im\n\n\n(3) \\(\\hat{\\bf p}_{ls}=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\)\n\nĉₓ = vec(x*x')\np̂ = inv(Gₙₚ' * Gₙₚ) * Gₙₚ' * ĉₓ \n\n100-element Vector{ComplexF64}:\n  0.003314209663374193 - 2.7356277964988863e-19im\n   0.03651843533340838 - 4.01518191768058e-18im\n   0.03433532967764885 + 2.515448157755484e-17im\n 0.0016782856970223292 - 1.0070028487673847e-17im\n   0.10166837790613971 + 3.1129277935880596e-18im\n  0.015421439356883134 + 9.403422807142065e-18im\n  0.025184594035243472 - 3.993782799800785e-18im\n   0.04658258516955039 - 1.850761436988587e-18im\n   0.04658541677045607 + 1.1559103895961936e-17im\n  0.007878013503208905 + 3.559698092088507e-18im\n  0.023337481863133468 + 2.6204945155857973e-18im\n   0.02249381702313939 + 5.304406111488559e-18im\n  0.006724780595702225 - 1.655564138463681e-17im\n                       ⋮\n  0.006724780595702329 + 1.8121162053534517e-18im\n  0.022493817023139205 - 1.0461976779111972e-17im\n   0.02333748186313285 - 6.792203007975684e-18im\n  0.007878013503208907 - 2.3575339315335667e-18im\n  0.046585416770456294 + 1.5392042695643853e-17im\n  0.046582585169550106 - 1.123245521985718e-17im\n  0.025184594035242928 + 1.1628578774983873e-18im\n  0.015421439356883466 + 5.864828990948797e-18im\n   0.10166837790614085 + 2.2712943512935246e-17im\n 0.0016782856970221013 + 4.829637376114682e-18im\n   0.03433532967764831 + 3.3208196889839756e-19im\n  0.036518435333408754 + 1.3795822112205515e-17im\n\n\n\n?? 뭔가 스케일이 안맞음\n\n\nN^2 * p̂\n\n100-element Vector{ComplexF64}:\n  33.14209663374193 - 2.7356277964988864e-15im\n 365.18435333408377 - 4.0151819176805797e-14im\n  343.3532967764885 + 2.515448157755484e-13im\n 16.782856970223293 - 1.0070028487673847e-13im\n 1016.6837790613971 + 3.1129277935880596e-14im\n 154.21439356883135 + 9.403422807142065e-14im\n 251.84594035243472 - 3.9937827998007846e-14im\n  465.8258516955039 - 1.850761436988587e-14im\n  465.8541677045607 + 1.1559103895961937e-13im\n  78.78013503208905 + 3.559698092088507e-14im\n 233.37481863133468 + 2.6204945155857973e-14im\n 224.93817023139388 + 5.304406111488559e-14im\n  67.24780595702225 - 1.655564138463681e-13im\n                    ⋮\n  67.24780595702329 + 1.8121162053534517e-14im\n 224.93817023139206 - 1.0461976779111972e-13im\n  233.3748186313285 - 6.792203007975684e-14im\n  78.78013503208906 - 2.3575339315335666e-14im\n 465.85416770456294 + 1.5392042695643854e-13im\n 465.82585169550106 - 1.123245521985718e-13im\n 251.84594035242927 + 1.1628578774983874e-14im\n 154.21439356883465 + 5.864828990948797e-14im\n 1016.6837790614085 + 2.2712943512935246e-13im\n  16.78285697022101 + 4.8296373761146824e-14im\n  343.3532967764831 + 3.3208196889839758e-15im\n  365.1843533340875 + 1.3795822112205514e-13im\n\n\n\n\\(N^2\\)를 곱해주니까 아까부터 구하던 값이 그대로 잘 나옴. (\\({\\bf DFT}\\) 혹은 \\({\\bf GFT}\\)를 정의할때 \\(\\frac{1}{\\sqrt N}\\)으로 스케일링 하느냐 마느냐 차이때문에 생기는 현상임)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "href": "posts/2_Studies/GRAPH/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "의문점",
    "text": "의문점\n아래의 그림을 살펴보자.\n\n\n\n그림12.3(교재에서 긁어온 그림): Power spectral density estimation. All estimators are based on the same random process defined on the Karate club network (Zachary 1977). (A) Periodogram estimation with different numbers of observations. (B) Windowed average periodogram from a single realization and a different number of windows. (C) Windowed average periodogram for four windows and a varying number of realizations. (D) Parametric MA estimation for 1 and 10 realizations.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n이 그림은 다양한 방법으로 true PSD \\({\\bf p}\\)를 추정한 결과를 나타내는 PSD plot 이다25. 우리가 적용한 방법은 (A)에서 \\(R=1\\)일 경우이다. 보는것 처럼 true PSD 를 놀라울 정도로 제대로 추정하지 못한다26. 만약에 우리가 모형에서 하나의 시계열이 아니라 1000개의 정도의 시계열을 관측하였다면 좀 더 합리적으로 추정할 수 있다. 그런데 사실 하나의 모형에서 1000개씩이나 되는 시계열을 관측하는 일은 현실적으로 불가능하다27 따라서 우리는 비교적 적은 \\(R\\)에서 합리적인 PSD의 추정치를 이끌어내야 한다. 그림 (B),(C)는 상대적으로 적은 \\(R\\)에 대해 \\({\\bf p}\\)를 추정하는 windowed periodogram 을 이용하여 PSD를 추정한 결과이다. (C)를 살펴보면 \\(R=1\\) 일경우 \\({\\bf p}\\)를 추정한 값들이 나와있는데 (A)와 비교하면 꽤 합리적으로 보인다.25 x축이 freq, y축이 PSD26 맞추는게 없는 것 같은데?27 그리고 대부분 \\(R=1\\)이지..\n문제는 (A)-(C)에서 제안된 방법 모두가 (D)에 제시된 전통적인 방법에 비하여 퍼포먼스가 떨어진다는 것이다. (D)는 parametric 모형을 사용한 결과이다. 파라메트릭 방법이므로 특정 모델을 한정하고 거기에 대응하는 한두개의 모수만 추정하면 되므로 추정이 잘 된다.28 반면 (A)-(C)의 경우 한 두개의 파라메터가 아니라 \\({\\bf p}\\)의 모든 원소를 추정해야하므로 추정할 파라메터가 데이터의 수 \\(N\\)과 같다29. 따라서 추정치의 분산이 크다. 사실 이것은 파라메트릭 방법과 세미파라메트릭 방법이라는 구조적인 차이때문에 어쩔 수 없는 것 같다. 그래도 세미파라메트릭 방법은 머리아프게 모델링을 할 필요가 없고30 내가 적합한 모델이 맞는지 확인할 필요도 없다31는 장점이 있다.28 약간 바이어스가 있어보이긴 하는데, 우연히 생긴건지 이론적으로 항상 생기는 건지는 잘 모르겠네?29 이런걸 세미파라메트릭 모형이라고 해요30 그래서 플랏을 보면서 적당한 ARMA를 찾을 필요도 없고, AIC 니 BIC 를 따져가면서 모형선택을 할 필요도 없고31 적합이후에 잔차분석 같은거 안해도 된다는 의미\n아래는 나름 PSD를 추정하는 신기술인 것 같다.\n\n\n\n그림12.4(교재에서 긁어온 그림): PSD estimation from a subset of nodes. Estimators are based on a random process defined on the Karate club network (Zachary 1977). (A) Graph sampling for nonparametric PSD estimation. Here, 20 out of 34 nodes are observed. The sampled nodes are highlighted by the circles around the nodes. (B) Nonparametric PSD estimation based on observations from 20 nodes and 100 data snapshots. (C) Graph sampling for parametric MA PSD estimation. Here, 4 out of 34 nodes are observed. (D) Parametric MA PSD estimation based on observations from 4 nodes and 100 data snapshots.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n그래프신호의 sub-sampling을 이용하는 것 같은데 교재의 뒤쪽에 서술되어있다. \\(R=100\\)임을 고려하여도 퍼포먼스가 좋은 편인듯 하다32.32 내 생각엔 이게 핵심 기술인 것 같음"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-01-15-Chap-12.4.html",
    "href": "posts/2_Studies/GRAPH/2023-01-15-Chap-12.4.html",
    "title": "[CGSP] Chap 12.4: Node Subsampling for PSD Estimation",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics\n\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#49 (generic function with 1 method)\n\n\n\n12.4.1 The Sampling Problem\n아래와 같이 길이가 \\(N=10\\) 인 신호 \\({\\bf x}\\)를 고려하자.\n\nx = rand(10)\n\n10-element Vector{Float64}:\n 0.03235208758206609\n 0.5069925854414447\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n 0.24116013388795854\n 0.8439116925218157\n 0.6362602319916778\n 0.386069828675059\n 0.5313655894235898\n\n\n여기에서 1,3,4,5 번째 원소만 추출하여길이가 \\(K=4\\) 인 신호 \\({\\bf y}\\)를 만들고 싶다.\n\ny = x[[1,3,4,5]]\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n이 과정은 아래와 같이 수행할 수도 있다.\n\nΦ= [1 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0]\n\n4×10 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0\n 0  0  0  1  0  0  0  0  0  0\n 0  0  0  0  1  0  0  0  0  0\n\n\n\nΦ*x\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n즉 적당한 \\(K\\times N\\) selection matrix를 선언하여 subsampling을 수행할 수 있다. 이때 매트릭스 \\({\\bf \\Phi}\\)를 subsampling matrix 혹은 sparse sampling matrix 라고 부른다.\n\n\n12.4.2 Compressed LS Estimator\n\nN = 10\nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x) \n\n10×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n\n\n\nG = columnwise_kron(conj(V),V)\n\n100×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im   0.809017-0.587785im     …   0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n    ⋮                                ⋱  \n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.809017+0.587785im     …   0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0-1.11022e-16im          -1.0+2.27596e-15im\n 1.0+0.0im  -0.809017-0.587785im     …  -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n\n\n- 방법1\n\nĉx = vec(x*x')\np̂ = inv(G' * G) * G' * ĉx\n\n10-element Vector{ComplexF64}:\n    0.25854107856772546 + 2.245922875954761e-20im\n   0.004743491121735806 - 1.3138893409553828e-18im\n   0.006946482731189413 - 9.791191432641327e-19im\n   0.001721693617954179 - 1.9827974128203887e-18im\n   0.011344167525098774 + 2.6827005818057562e-19im\n 0.00012662617844242917 - 3.748573865136995e-20im\n   0.011344167525098762 + 2.7448152053954017e-18im\n  0.0017216936179541913 - 9.35534609073096e-19im\n   0.006946482731189404 + 1.954408900185458e-18im\n   0.004743491121735756 - 2.561030398375897e-18im\n\n\n- 방법2\n\nĉy = vec(y*y')\np̂ = (kron(Φ,Φ)*G)' * ĉy\n\n10-element Vector{ComplexF64}:\n   3.759462826821233 + 0.0im\n   2.765185174577697 - 2.0816681711721685e-17im\n   1.077337414764992 + 2.7755575615628914e-17im\n 0.11594812606807317 + 2.0816681711721685e-17im\n 0.08838298603932843 + 3.903127820947816e-17im\n 0.32863702713833354 + 4.622231866529366e-33im\n 0.08838298603932859 + 9.540979117872439e-18im\n  0.1159481260680729 - 2.0816681711721685e-17im\n  1.0773374147649915 + 0.0im\n  2.7651851745776965 - 2.0816681711721685e-17im"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Graph Signal.html",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Graph Signal.html",
    "title": "Graph Signal",
    "section": "",
    "text": "Summary\n\nthe given signal is observed on a graph \\({\\cal G}:=({\\cal V},{\\cal E})\\)\n\n\\({\\cal V}\\) represents the set of nodes\n\\({\\cal E}\\) represents the set of edges (links)\nwe have observed a real-valued signal, denoted as \\(y: V \\to \\mathbb{R}\\)\n\n\nImport\n\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\n\nGraph Signal\n\n\n\nFigure: A random positive graph signal on the vertices of the Petersen graph. The height of each blue bar represents the signal value at the vertex where the bar originates (Shuman et al. 2013)\nShuman, David I, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. “The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains.” IEEE Signal Processing Magazine 30 (3): 83–98.\n\n\n\nSuppose we have observed a real-valued signal, denoted as \\(y: V \\to \\mathbb{R}\\), from a graph \\({\\cal G} = (V, E)\\), where \\(V\\) represents the set of nodes and \\(E\\) represents the set of edges."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "using LinearAlgebra, FFTW"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Cyclic shfit operator \\({\\bf B}\\)",
    "text": "Cyclic shfit operator \\({\\bf B}\\)\nThe matrix \\({\\bf B}\\) representing the periodic shift is\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0]\n\n5×5 Matrix{Int64}:\n 0  0  0  0  1\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n\n\nThis matrix is the cyclic shift.\nnote: \\({\\bf B}\\) is orthogonal matrix.\n\nB'B\n\n5×5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n(ex1) Define \\({\\bf s}\\) as\n\ns = [1,2,3,4,5]\ns\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nObserve that\n\nB*s\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\n\nB^3*s\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 2\n\n\nThus we can interprete the matrix \\({\\bf B}\\) as cyclic shift operator such that\n\\[\n{\\bf B}s_n =s_{n-1}\n\\]\nfor \\(n=1,\\dots, N-1\\) and \\({\\bf B}s_0 =s_N\\).\nnote: \\({\\bf B}\\)는 시계열에서 다루는 backshift operator 와 비슷함."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\nThe matrix \\({\\bf B}\\) can be expressed as\n\\({\\bf B}={\\bf DFT}^\\ast \\cdot {\\bf \\Lambda} \\cdot {\\bf DFT}\\)\nwhere \\({\\bf DFT}\\) is unitary and symmetric matrix and \\(\\bf \\Lambda\\) is diagonal matrix.\n\nλ, Ψ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5×5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  …  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB ≈ Ψ * Diagonal(λ) * Ψ'\n\ntrue\n\n\nDefine \\({\\boldsymbol \\Psi}^\\ast={\\bf DFT}\\).\n\nDFT = Ψ'\n\n5×5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  …  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\nNote that the eigenvalues are not ordered in julia.\n\nλ[5], exp(-im* 2π/5 * 0)\n\n(0.9999999999999998 + 0.0im, 1.0 - 0.0im)\n\n\n\nλ[3], exp(-im* 2π/5 * 1)\n\n(0.30901699437494734 - 0.9510565162951536im, 0.30901699437494745 - 0.9510565162951535im)\n\n\n\nλ[1], exp(-im* 2π/5 * 2)\n\n(-0.8090169943749472 - 0.5877852522924725im, -0.8090169943749473 - 0.5877852522924732im)\n\n\n\nλ[2], exp(-im* 2π/5 * 3)\n\n(-0.8090169943749472 + 0.5877852522924725im, -0.8090169943749475 + 0.587785252292473im)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral components and Frequencies",
    "text": "Spectral components and Frequencies\nWe remark:\n(1) Spectral components: For \\(k = 0,1,2,\\dots, N-1\\), the \\(k\\)-th column of \\({\\bf DFT}^\\ast\\) is defined by\n\\[\\Psi_k:=\\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 \\\\ e^{j\\frac{2\\pi}{N}k} \\\\ e^{j\\frac{2\\pi}{N}2k} \\\\ e^{j\\frac{2\\pi}{N}3k} \\\\  \\dots \\\\ e^{j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}.\\]\nNote that \\(\\Psi_k\\) can be also interpreted as \\(\\ell\\)-th eigenvector of \\({\\bf A}\\) correspoding \\(\\lambda_\\ell = e^{-j\\frac{2\\pi}{N}k}\\). Those eigenvectors\n\\[\\big\\{{\\bf 1},\\Psi_1,\\Psi_2, \\dots, \\Psi_{N-1}\\big\\}\\]\nform a complete orthonomal basis of \\(\\mathbb{C}^N\\). These vectors are called spectral components.\n(2) Frequencies: The diagonal entries of \\({\\bf \\Lambda}\\) are the eigenvalues of the time shift \\({\\bf B}\\). In Physics and in operator theory, these eigenvalues are the frequencies of the signal. In DSP it is more common to call frequencies\n\\[\\Omega_k=\\frac{-1}{2\\pi j}\\ln\\lambda_k=\\frac{-1}{2\\pi j}\\ln e^{-j \\frac{2\\pi}{N}k}=\\frac{k}{N}, \\quad k=0,1,2,\\dots,N-1.\\]\n\nThe \\(N\\) (time) frequencies \\(\\Omega_k\\) are all distinct, positive, equally spaced, and increasing from \\(0\\) to \\(\\frac{N-1}{N}\\). The spectral components are the complex exponential sinusiodal functions. For example, corresponding to the zero frequency is the DC spectral component (a vector whose entries are constant and all equal to \\(\\frac{1}{\\sqrt{N}}\\))."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft-1",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft-1",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\n일반적으로 우리가 알고있는 DFT1는 아래와 같다. (이 그림은 위키피디아에서 캡쳐한 것이다)1 discrete Fourier transform\n\n\n\n그림1: 위키에서 긁어온 DFT의 정의\n\n\n즉 DFT는 임의의 신호 \\(\\{{\\bf x}_n\\}:=x_0,x_1,\\dots,x_{N-1}\\)를 적당한 규칙2에 따라서 \\(\\{{\\bf X}_k\\}:=X_0,X_1,\\dots,X_{N-1}\\)로 바꾸는 변환을 이라고 이해할 수 있다. 이때 사용되는 적당한 규칙은 구체적으로 아래의 수식을 의미한다.2 \\(X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\)\n\\[X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\]\n그런데 매트릭스를 활용하면 위의 수식을 아래와 같이 표현할 수 있다.\n\\[\\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\dots \\\\ X_{N-1} \\end{bmatrix}\n=\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\dots \\\\ x_{N-1} \\end{bmatrix}\\]\n편의상 \\({\\bf X}\\)와 \\({\\bf x}\\)를 \\(N \\times 1\\) col-vec이라고 생각하고 DFT를 아래와 같은 matrix로 정의하자.\n\\[{\\bf DFT} = \\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n그러면\n\\[{\\bf X} = {\\bf DFT} \\cdot {\\bf x}\\]\n와 같이 표현할 수 있고 \\({\\bf x}\\)에서 \\({\\bf X}\\)로 바꾸는 과정을 단순히 \\({\\bf DFT}\\)행렬을 \\({\\bf x}\\)의 왼쪽에 곱하는 과정으로 이해할 수 있다.\n(참고) 사실 아래와 같이 \\({\\bf DFT}\\)를 정의하는 버전도 있다. (둘이 혼용해서 쓰인다)\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\n예제1 아래는 위키에서 긁어온 예제이다. 이 예제를 따라가보자.\n\n\n\n그림2: 위키에서 긁어온 예제이미지\n\n\n예제를 풀기위해서 우선 아래와 같은 벡터를 선언하다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n(풀이1)\n\\(4\\times 4\\)의 크기를 가지는 DFT행렬을 선언한다.\n(step1) 아래의 매트릭스 생성\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4×4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n(step2) _DFT의 각 원소에 함수 \\(f: x \\to \\exp(-i\\frac{2\\pi}{4}x)\\)를 취함\n\nf = x -> exp(-im * (2π/4) * x)\nDFT = _DFT .|> f\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n이제 \\({\\bf X}\\)를 구하면 아래와 같다.\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n위키의 답이 잘 나옴\n(풀이2)\n참고로 아래와 같이 패키지를 이용하여 구할 수도 있다.\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#inverse-dft",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#inverse-dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Inverse DFT",
    "text": "Inverse DFT\n앞으로는 \\({\\bf DFT}\\)를 아래와 같이 정의하자.\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\\({\\bf DFT}\\)행렬에는 몇 가지 특징이 있다.\n특징1: 유니터리행렬이다. 즉 \\({\\bf DFT}^\\ast \\cdot {\\bf DFT} = {\\bf DFT}^\\ast \\cdot{\\bf DFT} = {\\bf I}\\) 이다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nf = x -> exp(-im * (2π/4) * x)\nDFT = _DFT .|> f\nDFT # 아까의 예제의 DFT!\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT = (1/√4)*DFT # 새로운 DFT의 정의 \nDFT'DFT .|> round # 유니터리행렬임을 확인!\n\n4×4 Matrix{ComplexF64}:\n  1.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   1.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   1.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   1.0+0.0im\n\n\n특징2: \\({\\bf DFT}\\)는 대칭행렬이다. 따라서 이 행렬의 켤레전치는 DFT의 각 원소에서 단순히 \\(i=\\sqrt{-1}\\) 대신에 \\(-i\\) 를 넣은 것과 같다.\n특징1-2를 조합하면 아래와 같이 \\({\\bf DFT}\\)에서 \\(i\\) 대신에 \\(-i\\)를 넣은 행렬이 변환 DFT를 취소시킬 수 있음을 이해할 수 있다. 33 아래의 행렬은 \\({\\bf DFT}^\\ast\\) 혹은 \\({\\bf DFT}\\)의 conjugate matrix 혹은 \\({\\bf DFT}^{-1}\\)로 생각할 수 있음\n\\[\\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 1} & e^{i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 2} & e^{i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n행렬 \\({\\bf DFT}\\)를 discrete Fourier transform으로 생각했듯이 위의 행렬을 inverse discrete Fourier transform으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT의 또 다른 정의",
    "text": "DFT의 또 다른 정의\n이번에는 \\({\\bf DFT}\\)에 대한 다른 정의를 생각해보자. 우선 아래와 같은 행렬 \\({\\bf B}\\)를 고려하자.\n\nB= [0 0 0 1 \n    1 0 0 0 \n    0 1 0 0\n    0 0 1 0]\n\n4×4 Matrix{Int64}:\n 0  0  0  1\n 1  0  0  0\n 0  1  0  0\n 0  0  1  0\n\n\n이것은 길이가 4인 임의의 column vector를 아래로 한칸씩 이동시키는 매트릭스이다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\nB*x # 아래로 한칸이동 \n\n4-element Vector{Complex{Int64}}:\n -1 + 2im\n  1 + 0im\n  2 - 1im\n  0 - 1im\n\n\n\nB^2*x # 아래로 두칸이동, B^2*x = B*(Bx) 이므로 \n\n4-element Vector{Complex{Int64}}:\n  0 - 1im\n -1 + 2im\n  1 + 0im\n  2 - 1im\n\n\n한편 이 매트릭스 \\({\\bf B}\\)는 아래와 같이 고유분해가 가능하다.\n\\[ {\\bf B} = {\\bf \\Psi} {\\bf \\Lambda} {\\bf \\Psi}^\\ast\\]\n\n\\({\\bf \\Psi}\\): make \\(\\frac{1}{\\sqrt{N}}[e^{\\sqrt{-1} \\frac{2\\pi}{N} ij}~\\text{ for }~ i=0,1,2,\\dots,N-1~\\text{ for }~j=0,1,2,\\dots,N-1]\\) and apply reshape function with \\((N,N)\\).\n\\({\\bf \\Lambda}\\): make \\([e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}~\\text{ for }~ i=0,1,2\\dots,N-1]\\) and apply Diagonal function.\n\n\nN = 4 \nλ = [exp(-im * (2π/N) *i) for i in 0:(N-1)]\nΛ = Diagonal(λ)\n_Ψ = 1/√N *[exp(im * (2π/N) * i*j) for i in 0:(N-1) for j in 0:(N-1)]\nΨ = reshape(_Ψ, (N,N))\nB ≈ Ψ * Λ * Ψ'\n\ntrue\n\n\n그런데 위에서 정의된 \\({\\bf \\Psi}^\\ast\\)는 우리가 그전에 정의하였던 \\({\\bf DFT}\\)의 행렬과 같다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nDFT = _DFT .|> (x -> exp(-im * (2π/4) * x)) \nDFT = DFT * 1/√N\n\n4×4 Matrix{ComplexF64}:\n 0.5-0.0im           0.5-0.0im          …           0.5-0.0im\n 0.5-0.0im   3.06162e-17-0.5im             -9.18485e-17+0.5im\n 0.5-0.0im          -0.5-6.12323e-17im             -0.5-1.83697e-16im\n 0.5-0.0im  -9.18485e-17+0.5im              2.75546e-16-0.5im\n\n\n\nΨ' == DFT \n\ntrue\n\n\n결국 요약하면 길이가 \\(N\\)인 신호의 \\({\\bf DFT}\\)행렬은 아래의 과정으로 구할 수 있음을 알 수 있다.\n\nForward operator \\({\\bf A}\\)를 정의한다.\n\\({\\bf A}\\)의 고유벡터행렬 \\({\\bf \\Psi}\\)을 구한다. 4\n\\({\\bf \\Psi}\\)의 conjugate transpose matrix \\({\\bf \\Psi}^\\ast\\) 를 구한다. 이것이 \\({\\bf DFT}\\) matrix 이다. 5\n\n4 고유벡터행렬은 고유값 \\(e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}\\)에 의하여 정렬되어 있어야 함.5 사실 이미 대칭행렬이므로 conjugate matrix만 구하면 된다."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "href": "posts/2_Studies/GRAPH/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral component and Frequencies",
    "text": "Spectral component and Frequencies\n\\({\\bf A}\\)의 고유벡터 \\({\\bf \\Psi}\\)의 각 column을 spectral component라고 부른다.\n\nψ₁ = Ψ[:,1] # ψ₁ is first spectral component \nψ₂ = Ψ[:,2] # ψ₂ is seconde spectral component \nψ₃ = Ψ[:,3] # ψ₃ is third spectral component \nψ₄ = Ψ[:,4] # ψ₄ is last spectral component\n\n그리고 아래와 같은 수열을 \\(\\Omega_{k}=\\frac{k}{N}\\)을 frequency 라고 부른다.\n\nN=4 \nΩ = [k/N for k in 0:(N-1)]\nΩ\n\n4-element Vector{Float64}:\n 0.0\n 0.25\n 0.5\n 0.75"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html",
    "title": "Non-Euclidean vs Euclidean",
    "section": "",
    "text": "Regular Graph?\n\nSummary\n\n\nEuclidean data\n\nunderlying function이 regular graph로 정의가 가능한 data\nunderline(domain)이 euclidean domain에 위치한 데이터(x축, 1d-grid, 2d-grid 등)\neuclidean distance로 계산할 수 있는 data\n\nNon-Euclidean data\n\nunderlying function이 regular graph로 정의가 가능하지 않은 data\nunderline(domain)이 non-euclidean domain에 위치한 데이터(곡선, 곡면 등)\neuclidean distance calculation이 not reasonable한 data\n\n\n\n\n\n\n\n\nGraph vs Manifold\n\n\n\n\n굳이 포함관계를 따지자면, Non-Euclidean > Graph > Manifold\nNon-Euclidean\n\nGraph\n\n거리는 Edge 나 weight 로 정의함.\n\nManifold(ex. swiss roll, 아래 예시 있음!)\n\nunderline = domain(swiss roll에서 말린 곡면)\nunderlying function = color(swiss roll에서 무지개 색)\n유한한 그래프 시그널로 표현 가능\n\n무한한 노드에서 realization sample 뽑고,\n그래프로 가져오려면 distance 정의 수정이 필요하다.\n\n수정하는 방법\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)를 사용하여 가까운 것만 거리 계산하도록 하기, 곡선은 유클리디안 거리를 semi 사용하고(이 식에서 \\(\\kappa\\)로 먼 거리는 자르니까), 곡면은 하버사인 거리를 사용.\nsimilarity(유사도) 따지기(ex. 몇 번 건너서 다음 노드로 가는지 등)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#euclidean",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#euclidean",
    "title": "Non-Euclidean vs Euclidean",
    "section": "Euclidean",
    "text": "Euclidean\n\nEx1) 1D grid\n\nText, etc.\n\n\n\n\nFigure: Sentence, word, sound: 1D Euclidean domains. This image is sourced from the PAM Workshop “New Deep Learning Techniques” Feb 7th 2017\n\n\n\nw=np.zeros((5,5))\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif i-j == 1 : \n            w[i,j] = 1\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\n\n\n\n\n\nNote\n\n\n\n모든 Degree가 동일한, 특히 단위행렬로 나오는(Regular graph인) 유클리디안 데이터\n\n\n\nD\n\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(20, 5)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)\n\n\n\n\n\n\nEx2) 2d grids\n\nImage, etc.\n\n\n\n\nFigure: Image, volume, video: 2D, 3D, 2D+1 Euclidean domains. This image is sourced from the PAM Workshop “New Deep Learning Techniques” Feb 7th 2017\n\n\n\nw = np.ones((4, 4))\nfor i in range(4):\n    for j in range(4):\n        if i==j :\n            w[i,j] = 0\n\n\nlst = []\nfor i in range(4):\n    for j in range(4):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\n\n\n\n\n\nNote\n\n\n\n모든 Degree가 동일하여 \\(D = 3I\\)로 표현되는(Regular graph인) 유클리디안 데이터\n\n\n\nD\n\narray([[3., 0., 0., 0.],\n       [0., 3., 0., 0.],\n       [0., 0., 3., 0.],\n       [0., 0., 0., 3.]])\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#non-euclidean",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#non-euclidean",
    "title": "Non-Euclidean vs Euclidean",
    "section": "Non-Euclidean",
    "text": "Non-Euclidean\n\nEx3) Different Weights\n\nWeight 같다고 가정하고 그래프 시각화\n\n\nw=np.zeros((5,5))\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif i!=j: \n            w[i,j] = 1\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)\n\n\n\n\n\npi=np.pi\nang=np.linspace(-pi,pi-2*pi/5,5)\nr=5+np.cos(np.linspace(0,12*pi,5))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,5))\nf = f1 + np.random.normal(5)\n\n\nD = np.zeros([5,5])\nlocations = np.stack([vx, vy],axis=1)\nfor i in range(5):\n    for j in range(i,5):\n        D[i,j]=np.linalg.norm(locations[i]-locations[j])\nD = D + D.T\n\n\nD\n\narray([[ 0.        ,  6.0964895 , 11.4126782 ,  9.53062515,  7.05342303],\n       [ 6.0964895 ,  0.        ,  6.0964895 ,  7.60845213,  9.53062515],\n       [11.4126782 ,  6.0964895 ,  0.        ,  6.0964895 , 11.4126782 ],\n       [ 9.53062515,  7.60845213,  6.0964895 ,  0.        ,  6.0964895 ],\n       [ 7.05342303,  9.53062515, 11.4126782 ,  6.0964895 ,  0.        ]])\n\n\n\n\n\n\n\n\nNote\n\n\n\n가중치 값이 다 다르게 형성되어 있다. 따라서 \\(D=kI\\)형태로도 표현할 수 없어 레귤러 메트릭스의 정의를 충족하지 못하며, 이는 비유클리디안 데이터이다.\n\n\n\n\nEx3) Non-Euclidean data with Non-Euclidean domain\n\ndegree matrix가 단위행렬이 아니어서 레귤러 그래프가 아닌 그래프\n\n\n1. 3D shape, Manifold\n\n도메인이 표면(컵)이며, underlying function 이 regular graph로 정의되지 않는다.\n\nreference\n\n\n\nFigure: Surface (non-Euclid data). This image is sourced from the (Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University)\n\n\n\n\n2. 3D shape, Manifold\n\n도메인이 구로 형성되어 있고, graph로 인지 시, underlying function 이 색(파란색)으로 볼 수 있고, 다른 색으로 구성된 것은 \\(\\eta\\)로 볼 수 있고, regular graph로 정의되지 않는다.\n\n간단 \\(\\eta\\) 정의 review = noise 이지만, 특정 i에서 값이 큰 nois를 갖음\n\n\n\nFigure: Distributions on 3D shapes (non-Euclid data). This image is sourced from the Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University\n\n\n\n\n3. 3D shape, Manifold\n\n도메인이 표면(고양이)이며, underlying function 이 색이라고 할 수 있다.\n\n\n\n\nFigure: Functions on manifolds (non-Euclid data). This image is sourced from the Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University\n\n\n\n\n4. 3D shape, Manifold\n\n도메인이 표면이며, underlying function 이 색이다. graph로 볼 때 regular graph로 정의되지 않는다.\n\nreference\n\n\n\nFigure: General manifolds (non-Euclid data). This image is sourced from the (Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University)\n\n\n\n\n5. Graph\n\n도메인이 노드인 graph. underlying function 은 정의할 수 없지만 굳이 따지자면 Classification work. 신경망 모양..\n\nreference\n\n\n\nFigure: Graphs networks (non-Euclid data). This image is sourced from the [Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University]\n\n\n\n\n6. Graph(Manifold?)\n\n위의 정의 참고, 노드가 도메인인 graph. 색이 underlying function.\n\nSwiss roll (non-euclid data) from (Das and Pal 2021)\n\nDas, Suchismita, and Nikhil R Pal. 2021. “Nonlinear Dimensionality Reduction for Data Visualization: An Unsupervised Fuzzy Rule-Based Approach.” IEEE Transactions on Fuzzy Systems 30 (7): 2157–69.\n\n\n\nFigure: Swiss roll (non-euclid data) from Das and Pal (2021)\nDas, Suchismita, and Nikhil R Pal. 2021. “Nonlinear Dimensionality Reduction for Data Visualization: An Unsupervised Fuzzy Rule-Based Approach.” IEEE Transactions on Fuzzy Systems 30 (7): 2157–69.\n\n\n\n\n\n7. Graph\n\n도메인이 노드이며, classification work\n\n\n\n\nFigure: Brain network (non-Euclid data) from Ginestet, Fournel, and Simmons (2014)\nGinestet, Cedric E, Arnaud P Fournel, and Andrew Simmons. 2014. “Statistical Network Analysis for Functional MRI: Summary Networks and Group Comparisons.” Frontiers in Computational Neuroscience 8: 51.\n\n\n\n\n\n8. Graph Signal\n\n도메인이 노드(택시 탄 장소)이며, underlying function 이 색(택시 픽업 얼마나 하는지를 나타냄, 많이 할 수록 레드쪽으로) regular graph로 정의되지 않는다.\n\n\n\n\nFigure: Taxi-pickup distribution in Manhattan (non-euclid data) from Ortega et al. (2018)\nOrtega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” Proceedings of the IEEE 106 (5): 808–28.\n\n\n\n\n\n9. Graph signal, spatiotemporal data\n\n도메인이 노드(sequence)이며, underlying function 이 regular graph로 정의되지 않는다.(파란색인 양의 signal, 검정색인 음의 signal로 mapping되어 있는 graph signal의 형태)\n\n\n\n\nFigure: Minnesota road graph (non-euclid data) from Shuman et al.(2013)\n\n\n\n\n10. Graph(Sequence), dynamic spatiotemporal data\n\n도메인이 표면(사람,motion을 sequence로 전달)이며, dynamic spatiotemporal data\n\n\n\n\nFigure: 3D point cloud sequence (non-euclid data) from (Ortega et al. 2018)\nOrtega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” Proceedings of the IEEE 106 (5): 808–28."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html",
    "title": "Regular Graph",
    "section": "",
    "text": "summary\n모든 노드가 동일한 degree를 갖는 그래프, degree matrix가 \\(I\\)나 \\(kI\\)로 나타낼 수 있는 그래프\ndegree matrix: the degree matrix of an undirected graph is a diagonal matrix which contains information about the degree of each vertex—that is, the number of edges attached to each vertex."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-1",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-1",
    "title": "Regular Graph",
    "section": "0-regular graph",
    "text": "0-regular graph\n\n참고 아래도 동일 차수이므로 regular graph\n\n\nw = np.array([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(I\\)\n\n\nD\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-2",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-2",
    "title": "Regular Graph",
    "section": "1-regular graph",
    "text": "1-regular graph\n\nw = np.array([\n       [0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(I\\)\n\n\nD\n\narray([[1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 1.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-3",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-3",
    "title": "Regular Graph",
    "section": "2-regular graph",
    "text": "2-regular graph\n\nw = np.array([\n       [0., 1., 1., 0., 0., 0.],\n       [1., 0., 1., 0., 0., 0.],\n       [1., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 1.],\n       [0., 0., 0., 1., 0., 1.],\n       [0., 0., 0., 1., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(2I\\)\n\n\nD\n\narray([[2., 0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0., 0.],\n       [0., 0., 0., 2., 0., 0.],\n       [0., 0., 0., 0., 2., 0.],\n       [0., 0., 0., 0., 0., 2.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-4",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-4",
    "title": "Regular Graph",
    "section": "3-regular graph",
    "text": "3-regular graph\n\nw = np.array([\n       [0., 1., 1., 0., 0., 1.],\n       [1., 0., 1., 1., 0., 0.],\n       [1., 1., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 1.],\n       [1., 0., 0., 1., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(3I\\)\n\n\nD\n\narray([[3., 0., 0., 0., 0., 0.],\n       [0., 3., 0., 0., 0., 0.],\n       [0., 0., 3., 0., 0., 0.],\n       [0., 0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 3., 0.],\n       [0., 0., 0., 0., 0., 3.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/3_Researches/ITTGNN/2023-05-18-Self Consistency toy ex.html",
    "href": "posts/3_Researches/ITTGNN/2023-05-18-Self Consistency toy ex.html",
    "title": "Self Consistency Toy ex",
    "section": "",
    "text": "Self Consistency\nRef: Self Consistency: A General Recipe for Wavelet Estimation With Irregularly-spaced and/or Incomplete Data\n\\[\\mathbb{E}(\\hat{f}_{com} | f = \\hat{f}_{obs}) = \\hat{f}_{obs}\\]"
  },
  {
    "objectID": "posts/3_Researches/ITTGNN/2023-05-18-Self Consistency toy ex.html#self-consistency-how-does-it-work",
    "href": "posts/3_Researches/ITTGNN/2023-05-18-Self Consistency toy ex.html#self-consistency-how-does-it-work",
    "title": "Self Consistency Toy ex",
    "section": "2 Self Consistency: How Does It Work?",
    "text": "2 Self Consistency: How Does It Work?\n\n2.1 Self-consistency: An Intuitive Principle\n책에서의 가정\n\n\\(x = 0,1,2,3,\\dots,16\\)으로 fixed 되어 있음.\n\\(y_0,\\dots, y_{13}\\)까지의 값을 알고 있는데 \\(y_{14},y_{15},y_{16}\\)의 값은 모른다.\n\n\\(y_i=\\beta x_i + \\epsilon_i, i=1,\\dots,n, \\epsilon_i \\sim \\text{i.i.d.}F(0,\\sigma^2)\\)\n\n\\(\\beta\\)의 최소제곱추정치\n\n\\(\\hat{\\beta}_n = \\hat{\\beta}_n(y_1 ,\\dots,y_n) = \\frac{\\sum^n_{i=1} y_i x_i}{\\sum^n_{i=1} x_i^2}\\)\n\n단, \\(m<n\\) 이고, \\(\\sum ^n_{m+1} x_i^2 > 0\\) 일 때,\n\n\\(E(\\hat{\\beta}_n|y_a, \\dots,y_m,;\\beta = \\hat{\\beta}_m) = \\hat{\\beta}_m\\)\nthe least-squares estimator has a (Martingale-like property)1, and reaches a perfect equilibrium in its projective properties1 확률 과정 중 과거의 정보를 알고 있다면 미래의 기댓값이 현재값과 동일한 과정\n참고;(위키백과)[https://ko.wikipedia.org/wiki/%EB%A7%88%ED%8C%85%EA%B2%8C%EC%9D%BC]\n\n\\(\\beta_n\\)을 구한다.\n\\(\\beta_n \\times x\\) 를 구한다.\nmissing 값이 있는 index만 대체한다.\n다시 \\(\\beta_n\\)을 구한다.\n.. 반복\n\n\\(\\beta\\)의 선형성 때문에 가능한 이론\n아래 계산하면 맞아야 함\n\\(\\hat{\\beta}_n = \\frac{\\sum_{i=1}^m y_i x_i + \\hat{\\beta}_m \\sum_{i=m+1}^n x_i^2}{\\sum_{i=1}^n x_i^2}\\)\n\n\n2.2 A Self–consistent Regression Estimator\n목적은 최적의 \\(\\hat{f}_{com}\\) 찾는 것, 일단 이 paper는 웨이블릿에 중점을 두고 비모수, 준모수 회귀로 확장 가능 누적 분포 함수 CDF 찾는 것이 목적"
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-07-01-graph_spectral_domain.html",
    "href": "posts/3_Researches/GODE/2023-07-01-graph_spectral_domain.html",
    "title": "Graph and Spectral domain",
    "section": "",
    "text": "Graph domain에서 Spectral domain으로, 아니면 그 반대로\n\n\ngraph Lebesgue decomposition theorem\n\\[F = F_{ac} + F_{sc} + F_{pp}\\]\n\n\\(F\\)가 미분 가능하다는 말은 \\(p\\)1이 존재하다는 말이 된다. 즉, \\(F = p\\)\n\\(y\\)가 weakly stationary하다 \\(\\to\\) \\(p\\)가 존재한다. \\(\\to\\) \\(F\\)가 미분가능하다. \\(\\to\\) \\(F_{sc} = 0\\)이다.\nWold representation theorem2에 따라 임의의 시계열 \\(y_t\\)는 deterministic term(predictable term)3과 stocastic term4으로 나눌 수 있고, 이 때 stocastic term은 무한 차수의 MA로 나타낼 수 있다.\n\n연구에서 \\(V^H y_t = V^H \\text{결정적 성분} + V^H \\text{확률적 성분}\\)\n확률적 성분을 무한차수의 MA는 GFT하면 \\(p_{ac}\\)가 나온다.5\n\n\n1 periodogram2 time domain에서만 정의된3 결정적 성분4 확률적 성분5 이미 알려진 사실..\\[p = p_{ac} + p_{pp}\\]\n\\(F\\)가 증가함수 일때, \\(F_{sc}\\)는 라돈니코딤 도함수(미분)에 의해 \\(F_{sc}\\)이 없어진다.66 0이 되기 때문에\n\n\nAppendix 1 르벡분해정리\n- Thm: 분포함수의 정의7를 만족하는 임의의 \\(F\\)는 항상 아래와 같이 분해가능하다.7 증가함수\n\\[F = F_{ac}+F_{pp}+F_{sing}\\]\n여기에서 \\(F_{ac}\\)는 르벡메져에 대하여 절대연속이고 \\(F_{pp}\\)는 카운팅메져에 대하여 절대연속이다. 따라서 \\(F_{ac}\\)와 \\(F_{pp}\\)는 각각 르벡메져와 카운팅메져에 대응하는 밀도함수가 존재한다. \\(F_{sing}\\)는 칸토어분포와 같이 밀도함수가 존재하지 않는 경우이다.\n\n여기에서 \\(ac\\)는 absolutely continuous 의 약자이고, \\(pp\\) pure point 의 약자이며 \\(sing\\)은 singular continuous 약자이다.\n\n- 의미: \\(F_{ac}\\)는 우리가 일반적으로 생각하는 singular하지 않은 연속함수를 상상하면 된다.8 \\(F_{pp}\\)는 완벽한 불연속이며 오직 jump를 통해서만 증가하는 함수라 생각하면 된다. 즉 우리가 익숙한 이산형확률변수의 cdf를 상상하면 된다.8 칸토어처럼 이상한 연속함수 말고 상식적인 수준의 연속함수라는 의미\n- 이론: \\(F_{pp}\\)는 기껏해야 countable한 불연속점을 가진다. (jump 하는 point는 countable이라는 의미, 결국 이산형확률변수의 support는 countable이라는 의미)\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{ac}\\]\n\\(F\\)에 대응하는 연속형 확률변수 \\(X\\)가 존재하고 그에 대응하는 pdf가 존재한다.\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{pp}\\]\n\\(F\\)에 대응하는 이산형 확률변수 \\(X\\)가 존재하고 그에 대응하는 (일반화된) pdf 혹은 pmf가 존재한다.\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{ac}+F_{pp}\\]\n\\(F\\)에 대응하는 혼합형 확률변수 \\(X\\)가 존재하고 그에 대응하는 (일반화된) pdf가 존재한다.\n\n\nAppendix 2 라돈니코딤 정리\n- 이론: 분포함수 \\(F_X:\\mathbb{R} \\to [0,1]\\)가 (르벡메져에 대하여) 절대연속이라면 아래를 만족하는 함수 \\(f_X:\\mathbb{R} \\to \\mathbb{R}^+\\)가 존재한다.\n\\[F_X = \\int_{(-\\infty,x]}f_Xd\\lambda\\]\n여기에서 함수 \\(f_X\\)를 \\(F_X\\)의 밀도함수 (density function) 이라고 한다. 일반적으로 밀도함수 \\(f_X\\)는 유일하지 않지만, 르벡측도로 재었을때 0인 집합을 제외한 부분에서는 유일하게 결정된다. (요약: 분포함수 \\(F_X\\)가 절대연속이면 밀도함수 \\(f_X\\)가 존재하고, 거의 유일함)\n\n위에서 “르벡측도로 재었을때 0인 집합을 제외한 부분에서는 유일하게 결정된다”라는 부분은 “르벡메져 \\(\\lambda\\)에 대하여 거의 유일하다” 라고 이해해도 무방. 엄밀하게 쓰면 “분포함수 \\(F_X\\)가 있다면 밀도함수의 정의하는 만족하는 함수가 반드시 하나는 존재한다. 만약에 두 함수 \\(f\\)와 \\(g\\)가 모두 밀도함수의 정의를 만족한다면 ‘\\(f=g\\) a.e. with respect to \\(\\lambda\\)’ 가 성립한다.” 와 같은 식으로 쓸 수 있음.\n\n\n위에서 \\(f\\)의 공역이 \\(\\mathbb{R}^+\\)인 이유는 \\(F_X\\)가 증가함수라서..\n\n- Thm (라돈니코딤 정리)(Durrett 2019, Thm A.4.8.): 가측공간 \\((S,{\\cal S})\\)를 고려하자. 그리고 \\(\\mu\\)와 \\(\\lambda\\)가 \\((S,{\\cal S})\\)에서의 \\(\\sigma\\)-finite measure 라고 하자. 만약에 \\(\\mu << \\lambda\\) 이라면 아래를 만족하는 가측함수 \\(f:(S,{\\cal S}) \\to (\\mathbb{R}^+,{\\cal R}^+)\\)가 거의 유일하게 (w.r.t. \\(\\lambda\\)) 존재한다.\n\nDurrett, Rick. 2019. Probability: Theory and Examples. Vol. 49. Cambridge university press.\n\\[\\forall B \\in {\\cal S}:~ \\mu(B) = \\int_B f d\\lambda.\\]\n여기에서 \\(f\\)를 Radon-Nikodym derivative of \\(\\mu\\) w.r.t. \\(\\lambda\\) 라고 하며, 이러한 의미에서 \\(f=\\frac{d\\mu}{d\\lambda}\\)와 같이 표현하기도 한다."
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-07-01-NonEuclidean_data_of_GODE.html",
    "href": "posts/3_Researches/GODE/2023-07-01-NonEuclidean_data_of_GODE.html",
    "title": "Non-Euclidean data of GODE",
    "section": "",
    "text": "GODE에서 사용된 Graph signal 예제들에 대한 설명\n\n\nImport\n\nimport pickle\n\n\n\nEsmaple 1. Simple Linear\nData Information\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}:=\\{(x_1,y_2,z_3),\\dots,(x_n,y_n,z_n)\\}\\)\n\n\\(r_i= 5 + \\cos(\\frac{12\\pi (i - 1)}{n - 1})\\), \\(\\theta_i= -\\pi + \\frac{{\\pi(n-2)(i - 1)}}{n(n - 1)}\\)\n\\(x_i = r_i \\cos(\\theta_i)\\), \\(y_i = r_i \\sin(\\theta_i)\\), \\(z_i = 10 \\cdot \\sin(\\frac{{6\\pi \\cdot (i - 1)}}{{n - 1}})\\).\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)\n\n\nNote that \\({\\bf L}\\) is GSO, and in this case, GFT is just Discrete Fourier Transform also note that \\({\\cal G}_W\\) is a regular graph since \\({\\bf D}={\\bf I}\\). Thus this data is Euclidean data.\n\ngraph signal\n\\(y:V \\to \\mathbb{R}\\)\n\n\\(y_i=\\frac{10}{n}v_i+\\eta_i+\\epsilon_i\\)\n\\(\\epsilon_i \\sim N(\\mu,\\sigma^2)\\)\n\\(\\eta_i \\sim U^\\star\\) with sparsity \\(0.05\\) where \\(U^\\star\\) is mixture of \\(U(1.5,2)\\) and \\(U(-2,-1.5)\\).\n\n\nNote that \\({\\bf y}\\) is weakly stationary w.r.t. \\({\\bf L}\\).\n\nEuclidean Data\n\ndomain 이 \\(x\\)축(실수 \\(\\mathbb{R}\\)로 정의되는)인 유클리디안 데이터\nunderline이 선, 아래에서는 \\(y=5x\\)가 되겠다.\n\n1d-grid로 볼 수 있음.(non-euclidean vs euclidean 참고)\n\nunderliying function이 regular graph로 정의(regular graph 참고)\n\n\n\n\nFigure: First example of GODE\n\n\n\n\nEsmaple 2. Orbit\nData Information\n\\({\\cal G}_W=(V,E,{\\bf W})\\)\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}:=\\{(x_1,y_2,z_3),\\dots,(x_n,y_n,z_n)\\}\\)\n\n\\(r_i= 5 + \\cos(\\frac{12\\pi (i - 1)}{n - 1})\\), \\(\\theta_i= -\\pi + \\frac{{\\pi(n-2)(i - 1)}}{n(n - 1)}\\)\n\\(x_i = r_i \\cos(\\theta_i)\\), \\(y_i = r_i \\sin(\\theta_i)\\), \\(z_i = 10 \\cdot \\sin(\\frac{{6\\pi \\cdot (i - 1)}}{{n - 1}})\\).\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)\n\nNon-Euclidean Data\n\n2D shape이다.\ndomain이 곡선인 논유클리디안 데이터\nunderlying function이 regular graph로 정의되지 않는다.(regular graph 참고)\n거리 계산을 유클리드 거리로 할 때, underline이 곡선이라 합리적이지 않다.\nweight은 유클리디안 거리로 정의되어 있지만, \\(\\kappa\\)로 hyperparameter 지정해주어 거리가 짧으면 유클리디안 거리로 정의하고, 멀면 0으로 연결을 끊는 행렬으로 정의.\n\n\n\n\n\n\n\nNote\n\n\n\n유클리디안으로 보고 싶다면??\n\n모든 연결 weight를 끊어버리면 된다.\n그러면 regular graph 로 정의 가능.\n하지만, d연구의 목적에 어긋남.\n\n\n\n\nNote that \\({\\bf W}\\) is GSO, since \\({\\bf W}^\\top = {\\bf W}\\). In this cases, \\({\\cal G}_W\\) is not regular since there does not exist \\(k\\) such that \\({\\bf D}=k{\\bf I}\\).\n\ngraph signal \\({\\bf y}:V \\to \\mathbb{R}^3\\)\n\n\\({\\bf y}_i={\\boldsymbol v}_i+{\\boldsymbol \\eta}_i+{\\boldsymbol \\epsilon}_i\\)\n\\({\\boldsymbol \\epsilon}_i \\sim N({\\boldsymbol \\mu},\\sigma^2{\\bf I})\\)\n\\({\\boldsymbol \\eta}_i \\sim U^\\star\\) with sparsity \\(0.05\\) where \\(U^\\star\\) is mixture of \\(U(5,7)\\) and \\(U(-7,-5)\\).\n\n\nClearly \\({\\bf y}\\) is stationary w.r.t. \\({\\bf W}\\) or \\({\\bf L}\\).\n\n\n\n\nFigure: Second example of GODE\n\n\n\n\nEsmaple 3. Stanford Bunny\nData Information\npygsp 라이브러리를 사용하여 데이터 가져옴, weight도!(mesh 로 색의 퍼짐 정도로 나타낸 것으로 간단 이해)\nNon-Euclidean Data\n\n3D shape\ndomain이 곡면이고, underline이 곡면인 넌유클리디안 데이터\nunderlying function 이 색\n\n아래를 설명해보자면, 연한 파란-연한 초록 점이 언더라인 펑션으로 형성되어 있을때, 진한 색의 점들이 noise로 형성되어 있음.\n\nunderline이 곡면이라 유클리디안 거리를 사용하는 것이 합리적이지 않다.\n\n\n\n\nFigure: Third example of GODE\n\n\n\n\nReal data. Earthquake\nData Information\nThis data is actual data collected from USGS1 during the period from 2010 to 2014.1 https://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics\n\\({\\cal G}=(V,E)\\)\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}\\) where \\({\\boldsymbol v}:=({\\tt Latitude},{\\tt Longitude})\\)\n\\(W_{i,j} = \\begin{cases} \\exp(-\\frac{\\rho(i,j)}{2\\theta^2}) & if \\rho(i,j) \\le \\kappa \\\\ 0 & o.w. \\end{cases}\\)\n\nHere, \\(\\rho(i,j)=hs({\\boldsymbol v}_i, {\\boldsymbol v}_j)\\) is Haversine distance between \\({\\boldsymbol v}_i, {\\boldsymbol v}_j\\).\n\n\\(y_i\\) = magnitude\n\nNon-Euclidean Data\n\ndomain이 곡면이고, underline이 곡면인 넌유클리디안 데이터\nunderlying function 이 magnitude(지진 강도)\nhaversine 이용하면 곡면 거리가 이미 포함되어 있지만, 자료가 너무 많아 거리가 먼 연결을 끊어주는 역할로 hyperparameter인 \\(\\kappa\\)사용하였다.\n\n\nwith open(\"Figs/GODE_earthquake.pkl\", \"rb\") as file:\n    loaded_object = pickle.load(file)\n\nloaded_object"
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-05-20-EbayesThresh toy ex.html",
    "href": "posts/3_Researches/GODE/2023-05-20-EbayesThresh toy ex.html",
    "title": "EbayesThresh Toy ex",
    "section": "",
    "text": "Import\n\nfrom itstgcn.learners import * \n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd\n\n\nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nWhile \\({\\bf p}_y\\) serves as a consistent estimator for \\(\\mathbb{E}[|{\\bf V}^H{\\bf y}|^2]\\), it is not an efficient estimator, and therefore, improvement is needed (Djuric and Richard 2018). The traditional approach for improvement is to use the windowed periodogram.\nThe windowed periodogram is efficient in detecting specific frequencies or periods, but it may not be as efficient in estimating the underlying function. One notable paper that utilized the windowed periodogram is the one that detected the El Niño phenomenon.\nAs this structure exhibits a “sparse signal + heavy-tailed” characteristics, by applying Bayesian modeling and thresholding \\({\\bf p}_y\\), we can estimate an appropriate \\({\\bf p}_{pp}\\) as discussed in (Johnstone and Silverman 2004).\n\n\nDjuric, Petar, and Cédric Richard. 2018. Cooperative and Graph Signal Processing: Principles and Applications. Academic Press.\n\nJohnstone, Iain M, and Bernard W Silverman. 2004. “Needles and Straw in Haystacks: Empirical Bayes Estimates of Possibly Sparse Sequences.”\n\n\nBayesian Model\n\\(x_i \\sim N(\\mu_i,1)\\)\n확률변수가 잘 정의되어 있을때, 여기서 \\(\\mu_i\\)를 정하는 Baysian.\n\n\\(\\mu_i \\sim\\) 사전분포(\\(\\mu_i\\)를 뽑을 수 있는)\n\\((\\mu_i | X_i = x_i)^n_{i=1} \\sim\\) 사후분포\n\nex) \\(N(10,1) \\sim\\) 사전분포\n관측치\n\n_obs = [7.1,6.9,8.5]\n\n\nnp.mean(_obs)\n\n7.5\n\n\n관측치를 보니 평균이 10이 아닌 것 같다.\n\\(N(10-3,1) \\sim\\) 사후분포\n\n여기서, \\(10-3\\)이 posterior meman\n사후 분포를 정의할때, 이벤트의 mean이냐, median이냐로 잡는 방법은 정해진 것이 아니다.(이베이즈에서는 median으로 잡음)\n\nEbayes는 사전분포를 Heavy-tail으로 정의했다.\nheavy tail?\n\n\n\nimage.png\n\n\n\\(\\mu_x \\sim\\) Double Exponential \\(= p_{pp} + p_{ac}\\) -> 혼합형(misture) = pure point + absolutely continuous\n\\(E(\\mu_i | X_i = x_i) = \\hat{\\mu}_i\\) -> thresholding의 결과\n\\(f_{prior}(\\mu) = (1-w)\\delta_0(\\mu) + w \\gamma (\\mu)\\)\n\n\\(\\delta_0\\) = 디렉함수(특정값이 아니면 다 0으로 봄)\n\\(\\gamma = \\frac{a}{2} e^{-a|\\mu|}\\)\n\nEbayes의 역할 = 자동으로 \\(w\\)를 계산 혹은 추정\n\n\\(1-w\\) 확률로 \\(\\delta_0\\)를 정의, \\(w\\)의 확룔로 \\(\\gamma\\)를 정의.\n\n\\(X_i = \\mu_i + \\epsilon_i, \\epsilon_i \\sim N(0,1)\\)에서 \\(\\mu_i\\)를 찾는게 목적이다. 이게 바로 \\(\\eta\\)값\n\nEbayes로 sparse signal만 골러낼 것이다.\n평균 이상의 값에서 자를 것이다.\n\nthresh(임계치)를 잡는 게 어려울 텐데, 위에서 이베이즈가 \\(w\\)를 자동으로 잡아 확률 계산되는 방법론을 제안한 것,\n\nbaysian modeling 사용하여 heavy tail + impulse(sparse)에서 posterior median 추정하여 임계값thresh으로 \\(p\\)에서 \\(p_{pp}\\)를 추출하는 것이 GODE 목적\n\n\n\nEbayesThresh\n\nT = 100\n\n\nt = np.arange(T)/T * 10\n\n\ny_true = 3*np.sin(0.5*t) + 1.2*np.sin(1.0*t) + 0.5*np.sin(1.2*t) \n\n\ny = y_true + np.random.normal(size=T)\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y_true)\n\n\n\n\n- 관찰한 신호\n\nplt.plot(t,y,'o')\nplt.plot(t,y_true,'--')\n\n\n\n\n- 퓨리에 변환\n\nf = np.array(y)\nif len(f.shape)==1: f = f.reshape(-1,1)\nT,N = f.shape\nPsi = make_Psi(T)\nfbar = Psi.T @ f # apply dft \n\n\nplt.plot(t,fbar**2) # periodogram \n\n\n\n\n- threshed\n\nfbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\nplt.plot((fbar**2)) # periodogram \nplt.plot((fbar_threshed**2)) \n\n\n\n\n\nplt.plot((fbar**2)[20:80]) # periodogram \nplt.plot((fbar_threshed**2)[20:80]) \n\n\n\n\n- 역퓨리에변환\n\nyhat = Psi @ fbar_threshed # inverse dft\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y,'.')\nplt.plot(t,y_true,'--')\nplt.plot(t,yhat)\n\n\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(y,'.')\nplt.plot(y_true)\n\n\n\n\n\n\nResult\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(40,15))\n    fig.suptitle('Figure 1',fontsize=40)\n    \n    ax1.plot(y, 'b.',alpha=0.5)\n    ax1.plot(y_true,'p--',label='True')\n    ax1.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    \n    ax1.tick_params(axis='y', labelsize=20)\n    ax1.tick_params(axis='x', labelsize=20)\n    \n    ax2.plot(y, 'b.',alpha=0.5)\n    ax2.plot(y_true,'p--',label='True')\n    ax2.plot(yhat,label='y hat')\n    ax2.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot((fbar**2)) # periodogram \n    ax3.plot((fbar_threshed**2)) \n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    ax3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n    \n    ax4.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \n    ax4.plot(range(20, 80),(fbar_threshed**2)[20:80]) \n    ax4.set_xticks(range(20, 81, 10))\n    ax4.set_xticklabels(range(20, 81, 10))\n    # ax4.set_xticklabels(['20','40','60'])\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n\n\n\n\n\nfrom mpl_toolkits.axes_grid1.inset_locator import mark_inset, inset_axes\nplt.figure(figsize = (20,10))\nplt.suptitle('Figure',fontsize=40)\nax = plt.subplot(1, 1, 1)\nax.plot(range(0,100),(fbar**2))\nax.plot((fbar_threshed**2)) \naxins = inset_axes(ax, 8, 3, loc = 1, bbox_to_anchor=(0.8, 0.8),\n                   bbox_transform = ax.figure.transFigure)\naxins.plot(range(20, 80),(fbar**2)[20:80])\naxins.plot(range(20, 80),(fbar_threshed**2)[20:80]) \naxins.set_xlim(20, 80)\naxins.set_ylim(-0.1, 7)\nmark_inset(ax, axins, loc1=4, loc2=3, fc=\"none\", ec = \"0.01\")\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='x', labelsize=20)\naxins.tick_params(axis='y', labelsize=15)\naxins.tick_params(axis='x', labelsize=15)\n# plt.savefig('Ebayes_Toy.png')\n\n\n\n\n\nfrom matplotlib.patches import ConnectionPatch\nfig = plt.figure(figsize=(20,10))\nplt.suptitle('Figure 1',fontsize=40)\nplot1 = fig.add_subplot(2,2,(1,2))\n\nplot1.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \nplot1.plot(range(20, 80),(fbar_threshed**2)[20:80]) \nplot1.set_xticks(range(20, 81, 10))\nplot1.set_xticklabels(range(20, 81, 10))\nplot1.tick_params(axis='y', labelsize=20)\nplot1.tick_params(axis='x', labelsize=20)\n\nplot3 = fig.add_subplot(2,2,(3,4)) \n\nplot3.plot((fbar**2)) # periodogram \nplot3.plot((fbar_threshed**2)) \nplot3.tick_params(axis='y', labelsize=20)\nplot3.tick_params(axis='x', labelsize=20)\nplot3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n# plot3.fill_between((20, 80), 10, 60, facecolor= \"red\", alpha = 0.2)\nconn1 = ConnectionPatch(xyA = (20, -0.1), coordsA=plot1.transData,\n                       xyB=(20, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn1)\nconn2 = ConnectionPatch(xyA = (79, -0.1), coordsA=plot1.transData,\n                       xyB=(80, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn2)\nplt.show()\n\n\n\n\n\n\nIn article\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nimport rpy2\n\n\nfrom rpy2.robjects.packages import importr\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n#import rpy2\n#import rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nGNAR = importr('GNAR') # import GNAR \n#igraph = importr('igraph') # import igraph \nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n%%R\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n\n\\(X_i\\)에서 \\(\\mu_i\\) 추출 가능하는 것을 증명할 예제\n\n%%R\n# png(\"Ebayes_plot1.png\", width=1600, height=800)\npar(mfrow=c(1,2))\npar(cex.axis=2) \npar(cex.lab=2)\nplot(x, type='l', xlab=\"Observed data\", ylab=\"\")\nplot(ebayesthresh(x, sdev=1),type='l', xlab=\"Estimate\", ylab=\"\")\n# dev.off()\n\n\n\n\n\nimport itstgcn\n\n\nitstgcn.make_Psi(T)\n\narray([[ 0.07106691, -0.10050378,  0.10050378, ..., -0.10050378,\n        -0.10050378,  0.07106691],\n       [ 0.10050378, -0.14206225,  0.14184765, ...,  0.14184765,\n         0.14206225, -0.10050378],\n       [ 0.10050378, -0.14184765,  0.14099032, ..., -0.14099032,\n        -0.14184765,  0.10050378],\n       ...,\n       [ 0.10050378,  0.14184765,  0.14099032, ...,  0.14099032,\n        -0.14184765, -0.10050378],\n       [ 0.10050378,  0.14206225,  0.14184765, ..., -0.14184765,\n         0.14206225,  0.10050378],\n       [ 0.07106691,  0.10050378,  0.10050378, ...,  0.10050378,\n        -0.10050378, -0.07106691]])\n\n\ndef trim(f):\n    f = np.array(f)\n    if len(f.shape)==1: f = f.reshape(-1,1)\n    T,N = f.shape\n    Psi = make_Psi(T)\n    fbar = Psi.T @ f # apply dft \n    fbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\n    fhat = Psi @ fbar_threshed # inverse dft \n    return fhat\n\nplt.plot(y)\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T).T@y)\n\n\n\n\n\nplt.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T)@ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\n_T = 1000\n\n\n_t = np.arange(_T)/_T * 10\n\n\n_x = 1.5*np.sin(2*_t)+2*np.random.rand(_T)+1.5*np.sin(4*_t)+1.5*np.sin(8*_t)\nplt.plot(_x)\n\n\n\n\n\nimport itstgcn\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\n_node_ids = {'node1':0,'node2':1}\n\n_FX1 = np.stack([_x,_x],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n\ndata1 = pd.DataFrame({'x':_x,'x1':_x,'xer':_x,'xer1':_x})\n\n\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset = loader1.get_dataset(lags=2)\n\n\nmindex = itstgcn.rand_mindex(dataset,mrate=0.7)\ndataset_miss = itstgcn.miss(dataset,mindex,mtype='rand')\n\n\ndataset_padded = itstgcn.padding(dataset_miss,interpolation_method='linear')\n\n\nlrnr = itstgcn.StgcnLearner(dataset_padded)\n\n\nlrnr.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor = Eval_csy(lrnr,dataset_padded)\n\n\nlrnr_2 = itstgcn.ITStgcnLearner(dataset_padded)\n\n\nlrnr_2.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor_2 = Eval_csy(lrnr_2,dataset_padded)\n\nPsi\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fst.pdf', format='pdf')\n\n\n\n\nfourier transform\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem(itstgcn.make_Psi(_T).T@np.array(_x),linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem((itstgcn.make_Psi(_T).T@np.array(_x))[:100],linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd_zin.pdf', format='pdf')\n\n\n\n\nEbayesthresh/trim\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem((ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))))[:100],linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd_zout.pdf', format='pdf')\n\n\n\n\nfhat\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3,alpha=0.3)\n    ax1.plot(itstgcn.make_Psi(_T)@ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='Inverse Fourier Transform',lw=5)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fth.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax1.plot(data1['x'][:],'-',color='C3',label='Complete Data')\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_fst.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax2 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax2.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax2.plot(torch.cat([torch.tensor(data1['x'][:4]),torch.tensor(dataset_miss.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data',markersize=15)\n    ax2.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=40)\n    ax2.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_snd.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax3 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)    \n    ax3.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(evtor_2.f_tr[:,0],'--o',color='C3',alpha=0.8,label='Interpolarion')\n    ax3.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=40)\n    ax3.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_3rd.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax4 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax4.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(evtor.fhat_tr[:,0],color='brown',lw=3,label='STGCN')\n    ax4.plot(evtor_2.fhat_tr[:,0],color='blue',lw=3,label='ITSTGCN')\n    # ax4.plot(138, -1.2, 'o', markersize=230, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(220, -1.5, 'o', markersize=200, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(290, -1.2, 'o', markersize=310, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(455, -0.9, 'o', markersize=280, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=40)\n    ax4.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_4th_1.png')"
  },
  {
    "objectID": "3_ittgnn.html",
    "href": "3_ittgnn.html",
    "title": "ITTGNN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 18, 2023\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_essays.html",
    "href": "1_essays.html",
    "title": "Essays",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "2_graph.html",
    "href": "2_graph.html",
    "title": "Graph",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 2, 2023\n\n\nGraph Shift Operator\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean vs Euclidean\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nGraph Signal\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nRegular Graph\n\n\nSEOYEON CHOI\n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬\n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬\n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was generated to manage my blogs.\nThere are two purposes to achieving something here.\n\nFirst\nI will get definitions of any subjects and organize that in my words.\nIt is important to understand those definitions if I want to research exactly and improve myself.\n\n\nSecond\nI will brief my past and present researches.\nOverall, this blog will be the essential step in developing my skills."
  }
]