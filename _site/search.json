[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seoyeon",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 2, 2023\n\n\nGraph Shift Operator\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nGraph and Spectral domain\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean data of GODE\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean vs Euclidean\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nGraph Signal\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nRegular Graph\n\n\nSEOYEON CHOI\n\n\n\n\nMay 18, 2023\n\n\nEbayesThresh Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nMay 18, 2023\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_ml.html",
    "href": "2_ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-02-GSO.html",
    "href": "posts/2_Studies/GRAPH/2023-07-02-GSO.html",
    "title": "Graph Shift Operator",
    "section": "",
    "text": "Definition[@djuric2018cooperative]: Given a normal shift operator \\({\\bf S}\\), we say that a graph signal \\({\\bf y}\\) is weakly stationary with respect to \\({\\bf S}\\) if, for all \\(a\\), \\(b\\), and \\(c \\leq b\\), the following equality holds:\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf y}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf y}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf y}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf y} \\Big)^H \\bigg].\\]\n\n\n\n\n\n\nNote\n\n\n\nUsing \\({\\bf S}\\) as the periodic shift operator \\({\\bf S}=\\begin{cases} 1 & j-j = 1 \\\\ 0 & o.w.\\end{cases}.\\), the definition is equivalent to the traditional stationarity definition in time series analysis.\n\n\n\nConjugate\n\\(E(y)=0\\)을 가정하고, \\(y = (y_1,y_2)\\)의 벡터를 가정했을 때,\n\\(Cov(y) = \\begin{pmatrix} cov(y_1,y_2) & cov(y_1,y_2) \\\\ cov(y_2,y_1) & cov(y_2,y_2) \\end{pmatrix}\\)\n\n\\(cov(y_1,y_1) = V(y_1) = E(y_1 - \\mu_1)^2 = E(y_1)^2 (\\therefore \\mu = 0)\\)\n\\(cov(y_2,y_1) = E(y_2 - \\mu_2)E(y_1 - \\mu_1) = E(y_2-y_1)\\)\n\\(cov(y_2,y_2) = V(y_2) = E(y_2 - \\mu_2)^2 = E(y_2)^2 (\\therefore \\mu = 0)\\)\n\\(cov(y_1,y_2) = E(y_1 - \\mu_1)E(y_2 - \\mu_2) = E(y_1-y_2)\\)\n\n\\(= E \\begin{pmatrix} y_1^2 & y_1y_2 \\\\ y_2 y_1 & y_2^2 \\end{pmatrix} = E(y y^\\top)\\)\n\\(y = (y_q y_2)^\\top\\)\n\\(y^\\top = (y_1,y_2)\\)\n\\(y y^\\top = \\begin{bmatrix} y1 \\\\ y_2 \\end{bmatrix} \\begin{bmatrix} y2 & y_2 \\end{bmatrix} = \\begin{bmatrix} y_1^2 & y_1y_2 \\\\ y_2y_1 & y_2^2 \\end{bmatrix}\\)\n\\(cov(y) = E(y t^\\top) = E(y y^H)\\) -> 확률변수가 복소수일 경우 가정 가능하다\n\\(cov(y\\text{의 } a \\text{만큼 평행이동}) = cov(y\\text{의 } b \\text{만큼 평행이동})\\)\n\\(cov((S^ay)(S^b y)^\\top) = cov((S^c y)(S^d y)^\\top)\\)\n\n결국, normal GSO가 주어질 때, \\(y\\)는 약정상성을 S에 대해 가지고 있다는 말이 된다.\n정상성 조건: 평균, 분산이 일정할때, 자기 공분산이 시차 t에만 의존할 때"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Graph Signal.html",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Graph Signal.html",
    "title": "Graph Signal",
    "section": "",
    "text": "Summary\n\nthe given signal is observed on a graph \\({\\cal G}:=({\\cal V},{\\cal E})\\)\n\n\\({\\cal V}\\) represents the set of nodes\n\\({\\cal E}\\) represents the set of edges (links)\nwe have observed a real-valued signal, denoted as \\(y: V \\to \\mathbb{R}\\)\n\n\nImport\n\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\n\nGraph Signal\n\n\n\nFigure: A random positive graph signal on the vertices of the Petersen graph. The height of each blue bar represents the signal value at the vertex where the bar originates (Shuman et al. 2013)\nShuman, David I, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. “The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains.” IEEE Signal Processing Magazine 30 (3): 83–98.\n\n\n\nSuppose we have observed a real-valued signal, denoted as \\(y: V \\to \\mathbb{R}\\), from a graph \\({\\cal G} = (V, E)\\), where \\(V\\) represents the set of nodes and \\(E\\) represents the set of edges."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html",
    "title": "Non-Euclidean vs Euclidean",
    "section": "",
    "text": "Regular Graph?\n\nSummary\n\n\nEuclidean data\n\nunderlying function이 regular graph로 정의가 가능한 data\nunderline(domain)이 euclidean domain에 위치한 데이터(x축, 1d-grid, 2d-grid 등)\neuclidean distance로 계산할 수 있는 data\n\nNon-Euclidean data\n\nunderlying function이 regular graph로 정의가 가능하지 않은 data\nunderline(domain)이 non-euclidean domain에 위치한 데이터(곡선, 곡면 등)\neuclidean distance calculation이 not reasonable한 data\n\n\n\n\n\n\n\n\nGraph vs Manifold\n\n\n\n\n굳이 포함관계를 따지자면, Non-Euclidean > Graph > Manifold\nNon-Euclidean\n\nGraph\n\n거리는 Edge 나 weight 로 정의함.\n\nManifold(ex. swiss roll, 아래 예시 있음!)\n\nunderline = domain(swiss roll에서 말린 곡면)\nunderlying function = color(swiss roll에서 무지개 색)\n유한한 그래프 시그널로 표현 가능\n\n무한한 노드에서 realization sample 뽑고,\n그래프로 가져오려면 distance 정의 수정이 필요하다.\n\n수정하는 방법\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)를 사용하여 가까운 것만 거리 계산하도록 하기, 곡선은 유클리디안 거리를 semi 사용하고(이 식에서 \\(\\kappa\\)로 먼 거리는 자르니까), 곡면은 하버사인 거리를 사용.\nsimilarity(유사도) 따지기(ex. 몇 번 건너서 다음 노드로 가는지 등)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#euclidean",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#euclidean",
    "title": "Non-Euclidean vs Euclidean",
    "section": "Euclidean",
    "text": "Euclidean\n\nEx1) 1D grid\n\nText, etc.\n\n\n\n\nFigure: Sentence, word, sound: 1D Euclidean domains. This image is sourced from the PAM Workshop “New Deep Learning Techniques” Feb 7th 2017\n\n\n\nw=np.zeros((5,5))\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif i-j == 1 : \n            w[i,j] = 1\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\n\n\n\n\n\nNote\n\n\n\n모든 Degree가 동일한, 특히 단위행렬로 나오는(Regular graph인) 유클리디안 데이터\n\n\n\nD\n\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(20, 5)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)\n\n\n\n\n\n\nEx2) 2d grids\n\nImage, etc.\n\n\n\n\nFigure: Image, volume, video: 2D, 3D, 2D+1 Euclidean domains. This image is sourced from the PAM Workshop “New Deep Learning Techniques” Feb 7th 2017\n\n\n\nw = np.ones((4, 4))\nfor i in range(4):\n    for j in range(4):\n        if i==j :\n            w[i,j] = 0\n\n\nlst = []\nfor i in range(4):\n    for j in range(4):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\n\n\n\n\n\nNote\n\n\n\n모든 Degree가 동일하여 \\(D = 3I\\)로 표현되는(Regular graph인) 유클리디안 데이터\n\n\n\nD\n\narray([[3., 0., 0., 0.],\n       [0., 3., 0., 0.],\n       [0., 0., 3., 0.],\n       [0., 0., 0., 3.]])\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#non-euclidean",
    "href": "posts/2_Studies/GRAPH/2023-07-01-Euclidean_nonEuclidean.html#non-euclidean",
    "title": "Non-Euclidean vs Euclidean",
    "section": "Non-Euclidean",
    "text": "Non-Euclidean\n\nEx3) Different Weights\n\nWeight 같다고 가정하고 그래프 시각화\n\n\nw=np.zeros((5,5))\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif i!=j: \n            w[i,j] = 1\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)\n\n\n\n\n\npi=np.pi\nang=np.linspace(-pi,pi-2*pi/5,5)\nr=5+np.cos(np.linspace(0,12*pi,5))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,5))\nf = f1 + np.random.normal(5)\n\n\nD = np.zeros([5,5])\nlocations = np.stack([vx, vy],axis=1)\nfor i in range(5):\n    for j in range(i,5):\n        D[i,j]=np.linalg.norm(locations[i]-locations[j])\nD = D + D.T\n\n\nD\n\narray([[ 0.        ,  6.0964895 , 11.4126782 ,  9.53062515,  7.05342303],\n       [ 6.0964895 ,  0.        ,  6.0964895 ,  7.60845213,  9.53062515],\n       [11.4126782 ,  6.0964895 ,  0.        ,  6.0964895 , 11.4126782 ],\n       [ 9.53062515,  7.60845213,  6.0964895 ,  0.        ,  6.0964895 ],\n       [ 7.05342303,  9.53062515, 11.4126782 ,  6.0964895 ,  0.        ]])\n\n\n\n\n\n\n\n\nNote\n\n\n\n가중치 값이 다 다르게 형성되어 있다. 따라서 \\(D=kI\\)형태로도 표현할 수 없어 레귤러 메트릭스의 정의를 충족하지 못하며, 이는 비유클리디안 데이터이다.\n\n\n\n\nEx3) Non-Euclidean data with Non-Euclidean domain\n\ndegree matrix가 단위행렬이 아니어서 레귤러 그래프가 아닌 그래프\n\n\n1. 3D shape, Manifold\n\n도메인이 표면(컵)이며, underlying function 이 regular graph로 정의되지 않는다.\n\nreference\n\n\n\nFigure: Surface (non-Euclid data). This image is sourced from the (Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University)\n\n\n\n\n2. 3D shape, Manifold\n\n도메인이 구로 형성되어 있고, graph로 인지 시, underlying function 이 색(파란색)으로 볼 수 있고, 다른 색으로 구성된 것은 \\(\\eta\\)로 볼 수 있고, regular graph로 정의되지 않는다.\n\n간단 \\(\\eta\\) 정의 review = noise 이지만, 특정 i에서 값이 큰 nois를 갖음\n\n\n\nFigure: Distributions on 3D shapes (non-Euclid data). This image is sourced from the Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University\n\n\n\n\n3. 3D shape, Manifold\n\n도메인이 표면(고양이)이며, underlying function 이 색이라고 할 수 있다.\n\n\n\n\nFigure: Functions on manifolds (non-Euclid data). This image is sourced from the Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University\n\n\n\n\n4. 3D shape, Manifold\n\n도메인이 표면이며, underlying function 이 색이다. graph로 볼 때 regular graph로 정의되지 않는다.\n\nreference\n\n\n\nFigure: General manifolds (non-Euclid data). This image is sourced from the (Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University)\n\n\n\n\n5. Graph\n\n도메인이 노드인 graph. underlying function 은 정의할 수 없지만 굳이 따지자면 Classification work. 신경망 모양..\n\nreference\n\n\n\nFigure: Graphs networks (non-Euclid data). This image is sourced from the [Fall 2020 course website for Non-Euclidean Methods in Machine Learning (CS468), Stanford University]\n\n\n\n\n6. Graph(Manifold?)\n\n위의 정의 참고, 노드가 도메인인 graph. 색이 underlying function.\n\nSwiss roll (non-euclid data) from (Das and Pal 2021)\n\n\n\nFigure: Swiss roll (non-euclid data) from Das and Pal (2021)\nDas, Suchismita, and Nikhil R Pal. 2021. “Nonlinear Dimensionality Reduction for Data Visualization: An Unsupervised Fuzzy Rule-Based Approach.” IEEE Transactions on Fuzzy Systems 30 (7): 2157–69.\n\n\n\n\n\n7. Graph\n\n도메인이 노드이며, classification work\n\n\n\n\nFigure: Brain network (non-Euclid data) from Ginestet, Fournel, and Simmons (2014)\nGinestet, Cedric E, Arnaud P Fournel, and Andrew Simmons. 2014. “Statistical Network Analysis for Functional MRI: Summary Networks and Group Comparisons.” Frontiers in Computational Neuroscience 8: 51.\n\n\n\n\n\n8. Graph Signal\n\n도메인이 노드(택시 탄 장소)이며, underlying function 이 색(택시 픽업 얼마나 하는지를 나타냄, 많이 할 수록 레드쪽으로) regular graph로 정의되지 않는다.\n\n\n\n\nFigure: Taxi-pickup distribution in Manhattan (non-euclid data) from Ortega et al. (2018)\n\n\n\n\n9. Graph signal, spatiotemporal data\n\n도메인이 노드(sequence)이며, underlying function 이 regular graph로 정의되지 않는다.(파란색인 양의 signal, 검정색인 음의 signal로 mapping되어 있는 graph signal의 형태)\n\n\n\n\nFigure: Minnesota road graph (non-euclid data) from Shuman et al.(2013)\n\n\n\n\n10. Graph(Sequence), dynamic spatiotemporal data\n\n도메인이 표면(사람,motion을 sequence로 전달)이며, dynamic spatiotemporal data\n\n\n\n\nFigure: 3D point cloud sequence (non-euclid data) from (Ortega et al. 2018)\nOrtega, Antonio, Pascal Frossard, Jelena Kovačević, José MF Moura, and Pierre Vandergheynst. 2018. “Graph Signal Processing: Overview, Challenges, and Applications.” Proceedings of the IEEE 106 (5): 808–28."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html",
    "title": "Regular Graph",
    "section": "",
    "text": "summary\n모든 노드가 동일한 degree를 갖는 그래프, degree matrix가 \\(I\\)나 \\(kI\\)로 나타낼 수 있는 그래프\ndegree matrix: the degree matrix of an undirected graph is a diagonal matrix which contains information about the degree of each vertex—that is, the number of edges attached to each vertex."
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-1",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-1",
    "title": "Regular Graph",
    "section": "0-regular graph",
    "text": "0-regular graph\n\n참고 아래도 동일 차수이므로 regular graph\n\n\nw = np.array([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(I\\)\n\n\nD\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\nlst = []\nfor i in range(5):\n    for j in range(5):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-2",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-2",
    "title": "Regular Graph",
    "section": "1-regular graph",
    "text": "1-regular graph\n\nw = np.array([\n       [0., 1., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(I\\)\n\n\nD\n\narray([[1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 1.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-3",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-3",
    "title": "Regular Graph",
    "section": "2-regular graph",
    "text": "2-regular graph\n\nw = np.array([\n       [0., 1., 1., 0., 0., 0.],\n       [1., 0., 1., 0., 0., 0.],\n       [1., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 1.],\n       [0., 0., 0., 1., 0., 1.],\n       [0., 0., 0., 1., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(2I\\)\n\n\nD\n\narray([[2., 0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0., 0.],\n       [0., 0., 0., 2., 0., 0.],\n       [0., 0., 0., 0., 2., 0.],\n       [0., 0., 0., 0., 0., 2.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-4",
    "href": "posts/2_Studies/GRAPH/2023-06-30-Regular Graph.html#regular-graph-4",
    "title": "Regular Graph",
    "section": "3-regular graph",
    "text": "3-regular graph\n\nw = np.array([\n       [0., 1., 1., 0., 0., 1.],\n       [1., 0., 1., 1., 0., 0.],\n       [1., 1., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 1.],\n       [1., 0., 0., 1., 1., 0.]])\n\n\nd= w.sum(axis=1)\nD= np.diag(d)\n\n\nDegree matrix = \\(3I\\)\n\n\nD\n\narray([[3., 0., 0., 0., 0., 0.],\n       [0., 3., 0., 0., 0., 0.],\n       [0., 0., 3., 0., 0., 0.],\n       [0., 0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 3., 0.],\n       [0., 0., 0., 0., 0., 3.]])\n\n\n\nlst = []\nfor i in range(6):\n    for j in range(6):\n        if w[i,j] == 1:\n            lst.append([i,j])\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(np.array(lst))\n\n\nplt.figure(figsize=(10, 10)) \nnx.draw_networkx(G, with_labels=True, font_weight='bold', node_color='orange', node_size=1500, font_color='white', font_size=30,width=5)"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-05-18-Self Consistency toy ex.html",
    "href": "posts/3_Researches/ITSTGCN/2023-05-18-Self Consistency toy ex.html",
    "title": "Self Consistency Toy ex",
    "section": "",
    "text": "Self Consistency\nRef: Self Consistency: A General Recipe for Wavelet Estimation With Irregularly-spaced and/or Incomplete Data\n\\[\\mathbb{E}(\\hat{f}_{com} | f = \\hat{f}_{obs}) = \\hat{f}_{obs}\\]"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-05-18-Self Consistency toy ex.html#self-consistency-how-does-it-work",
    "href": "posts/3_Researches/ITSTGCN/2023-05-18-Self Consistency toy ex.html#self-consistency-how-does-it-work",
    "title": "Self Consistency Toy ex",
    "section": "2 Self Consistency: How Does It Work?",
    "text": "2 Self Consistency: How Does It Work?\n\n2.1 Self-consistency: An Intuitive Principle\n책에서의 가정\n\n\\(x = 0,1,2,3,\\dots,16\\)으로 fixed 되어 있음.\n\\(y_0,\\dots, y_{13}\\)까지의 값을 알고 있는데 \\(y_{14},y_{15},y_{16}\\)의 값은 모른다.\n\n\\(y_i=\\beta x_i + \\epsilon_i, i=1,\\dots,n, \\epsilon_i \\sim \\text{i.i.d.}F(0,\\sigma^2)\\)\n\n\\(\\beta\\)의 최소제곱추정치\n\n\\(\\hat{\\beta}_n = \\hat{\\beta}_n(y_1 ,\\dots,y_n) = \\frac{\\sum^n_{i=1} y_i x_i}{\\sum^n_{i=1} x_i^2}\\)\n\n단, \\(m<n\\) 이고, \\(\\sum ^n_{m+1} x_i^2 > 0\\) 일 때,\n\n\\(E(\\hat{\\beta}_n|y_a, \\dots,y_m,;\\beta = \\hat{\\beta}_m) = \\hat{\\beta}_m\\)\nthe least-squares estimator has a (Martingale-like property)1, and reaches a perfect equilibrium in its projective properties1 확률 과정 중 과거의 정보를 알고 있다면 미래의 기댓값이 현재값과 동일한 과정\n참고;(위키백과)[https://ko.wikipedia.org/wiki/%EB%A7%88%ED%8C%85%EA%B2%8C%EC%9D%BC]\n\n\\(\\beta_n\\)을 구한다.\n\\(\\beta_n \\times x\\) 를 구한다.\nmissing 값이 있는 index만 대체한다.\n다시 \\(\\beta_n\\)을 구한다.\n.. 반복\n\n\\(\\beta\\)의 선형성 때문에 가능한 이론\n아래 계산하면 맞아야 함\n\\(\\hat{\\beta}_n = \\frac{\\sum_{i=1}^m y_i x_i + \\hat{\\beta}_m \\sum_{i=m+1}^n x_i^2}{\\sum_{i=1}^n x_i^2}\\)\n\n\n2.2 A Self–consistent Regression Estimator\n목적은 최적의 \\(\\hat{f}_{com}\\) 찾는 것, 일단 이 paper는 웨이블릿에 중점을 두고 비모수, 준모수 회귀로 확장 가능 누적 분포 함수 CDF 찾는 것이 목적"
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-07-01-graph_spectral_domain.html",
    "href": "posts/3_Researches/GODE/2023-07-01-graph_spectral_domain.html",
    "title": "Graph and Spectral domain",
    "section": "",
    "text": "Graph domain에서 Spectral domain으로, 아니면 그 반대로\n\n\ngraph Lebesgue decomposition theorem\n\\[F = F_{ac} + F_{sc} + F_{pp}\\]\n\n\\(F\\)가 미분 가능하다는 말은 \\(p\\)1이 존재하다는 말이 된다. 즉, \\(F = p\\)\n\\(y\\)가 weakly stationary하다 \\(\\to\\) \\(p\\)가 존재한다. \\(\\to\\) \\(F\\)가 미분가능하다. \\(\\to\\) \\(F_{sc} = 0\\)이다.\nWold representation theorem2에 따라 임의의 시계열 \\(y_t\\)는 deterministic term(predictable term)3과 stocastic term4으로 나눌 수 있고, 이 때 stocastic term은 무한 차수의 MA로 나타낼 수 있다.\n\n연구에서 \\(V^H y_t = V^H \\text{결정적 성분} + V^H \\text{확률적 성분}\\)\n확률적 성분을 무한차수의 MA는 GFT하면 \\(p_{ac}\\)가 나온다.5\n\n\n1 periodogram2 time domain에서만 정의된3 결정적 성분4 확률적 성분5 이미 알려진 사실..\\[p = p_{ac} + p_{pp}\\]\n\\(F\\)가 증가함수 일때, \\(F_{sc}\\)는 라돈니코딤 도함수(미분)에 의해 \\(F_{sc}\\)이 없어진다.66 0이 되기 때문에\n\n\nAppendix 1 르벡분해정리\n- Thm: 분포함수의 정의7를 만족하는 임의의 \\(F\\)는 항상 아래와 같이 분해가능하다.7 증가함수\n\\[F = F_{ac}+F_{pp}+F_{sing}\\]\n여기에서 \\(F_{ac}\\)는 르벡메져에 대하여 절대연속이고 \\(F_{pp}\\)는 카운팅메져에 대하여 절대연속이다. 따라서 \\(F_{ac}\\)와 \\(F_{pp}\\)는 각각 르벡메져와 카운팅메져에 대응하는 밀도함수가 존재한다. \\(F_{sing}\\)는 칸토어분포와 같이 밀도함수가 존재하지 않는 경우이다.\n\n여기에서 \\(ac\\)는 absolutely continuous 의 약자이고, \\(pp\\) pure point 의 약자이며 \\(sing\\)은 singular continuous 약자이다.\n\n- 의미: \\(F_{ac}\\)는 우리가 일반적으로 생각하는 singular하지 않은 연속함수를 상상하면 된다.8 \\(F_{pp}\\)는 완벽한 불연속이며 오직 jump를 통해서만 증가하는 함수라 생각하면 된다. 즉 우리가 익숙한 이산형확률변수의 cdf를 상상하면 된다.8 칸토어처럼 이상한 연속함수 말고 상식적인 수준의 연속함수라는 의미\n- 이론: \\(F_{pp}\\)는 기껏해야 countable한 불연속점을 가진다. (jump 하는 point는 countable이라는 의미, 결국 이산형확률변수의 support는 countable이라는 의미)\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{ac}\\]\n\\(F\\)에 대응하는 연속형 확률변수 \\(X\\)가 존재하고 그에 대응하는 pdf가 존재한다.\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{pp}\\]\n\\(F\\)에 대응하는 이산형 확률변수 \\(X\\)가 존재하고 그에 대응하는 (일반화된) pdf 혹은 pmf가 존재한다.\n- 이론: 분포함수 정의를 만족하는 임의의 \\(F\\)가 아래와 같다면\n\\[F=F_{ac}+F_{pp}\\]\n\\(F\\)에 대응하는 혼합형 확률변수 \\(X\\)가 존재하고 그에 대응하는 (일반화된) pdf가 존재한다.\n\n\nAppendix 2 라돈니코딤 정리\n- 이론: 분포함수 \\(F_X:\\mathbb{R} \\to [0,1]\\)가 (르벡메져에 대하여) 절대연속이라면 아래를 만족하는 함수 \\(f_X:\\mathbb{R} \\to \\mathbb{R}^+\\)가 존재한다.\n\\[F_X = \\int_{(-\\infty,x]}f_Xd\\lambda\\]\n여기에서 함수 \\(f_X\\)를 \\(F_X\\)의 밀도함수 (density function) 이라고 한다. 일반적으로 밀도함수 \\(f_X\\)는 유일하지 않지만, 르벡측도로 재었을때 0인 집합을 제외한 부분에서는 유일하게 결정된다. (요약: 분포함수 \\(F_X\\)가 절대연속이면 밀도함수 \\(f_X\\)가 존재하고, 거의 유일함)\n\n위에서 “르벡측도로 재었을때 0인 집합을 제외한 부분에서는 유일하게 결정된다”라는 부분은 “르벡메져 \\(\\lambda\\)에 대하여 거의 유일하다” 라고 이해해도 무방. 엄밀하게 쓰면 “분포함수 \\(F_X\\)가 있다면 밀도함수의 정의하는 만족하는 함수가 반드시 하나는 존재한다. 만약에 두 함수 \\(f\\)와 \\(g\\)가 모두 밀도함수의 정의를 만족한다면 ‘\\(f=g\\) a.e. with respect to \\(\\lambda\\)’ 가 성립한다.” 와 같은 식으로 쓸 수 있음.\n\n\n위에서 \\(f\\)의 공역이 \\(\\mathbb{R}^+\\)인 이유는 \\(F_X\\)가 증가함수라서..\n\n- Thm (라돈니코딤 정리)[@durrett2019probability, Thm A.4.8.]: 가측공간 \\((S,{\\cal S})\\)를 고려하자. 그리고 \\(\\mu\\)와 \\(\\lambda\\)가 \\((S,{\\cal S})\\)에서의 \\(\\sigma\\)-finite measure 라고 하자. 만약에 \\(\\mu << \\lambda\\) 이라면 아래를 만족하는 가측함수 \\(f:(S,{\\cal S}) \\to (\\mathbb{R}^+,{\\cal R}^+)\\)가 거의 유일하게 (w.r.t. \\(\\lambda\\)) 존재한다.\n\\[\\forall B \\in {\\cal S}:~ \\mu(B) = \\int_B f d\\lambda.\\]\n여기에서 \\(f\\)를 Radon-Nikodym derivative of \\(\\mu\\) w.r.t. \\(\\lambda\\) 라고 하며, 이러한 의미에서 \\(f=\\frac{d\\mu}{d\\lambda}\\)와 같이 표현하기도 한다."
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-07-01-NonEuclidean_data_of_GODE.html",
    "href": "posts/3_Researches/GODE/2023-07-01-NonEuclidean_data_of_GODE.html",
    "title": "Non-Euclidean data of GODE",
    "section": "",
    "text": "GODE에서 사용된 Graph signal 예제들에 대한 설명\n\n\nImport\n\nimport pickle\n\n\n\nEsmaple 1. Simple Linear\nData Information\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}:=\\{(x_1,y_2,z_3),\\dots,(x_n,y_n,z_n)\\}\\)\n\n\\(r_i= 5 + \\cos(\\frac{12\\pi (i - 1)}{n - 1})\\), \\(\\theta_i= -\\pi + \\frac{{\\pi(n-2)(i - 1)}}{n(n - 1)}\\)\n\\(x_i = r_i \\cos(\\theta_i)\\), \\(y_i = r_i \\sin(\\theta_i)\\), \\(z_i = 10 \\cdot \\sin(\\frac{{6\\pi \\cdot (i - 1)}}{{n - 1}})\\).\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)\n\n\nNote that \\({\\bf L}\\) is GSO, and in this case, GFT is just Discrete Fourier Transform also note that \\({\\cal G}_W\\) is a regular graph since \\({\\bf D}={\\bf I}\\). Thus this data is Euclidean data.\n\ngraph signal\n\\(y:V \\to \\mathbb{R}\\)\n\n\\(y_i=\\frac{10}{n}v_i+\\eta_i+\\epsilon_i\\)\n\\(\\epsilon_i \\sim N(\\mu,\\sigma^2)\\)\n\\(\\eta_i \\sim U^\\star\\) with sparsity \\(0.05\\) where \\(U^\\star\\) is mixture of \\(U(1.5,2)\\) and \\(U(-2,-1.5)\\).\n\n\nNote that \\({\\bf y}\\) is weakly stationary w.r.t. \\({\\bf L}\\).\n\nEuclidean Data\n\ndomain 이 \\(x\\)축(실수 \\(\\mathbb{R}\\)로 정의되는)인 유클리디안 데이터\nunderline이 선, 아래에서는 \\(y=5x\\)가 되겠다.\n\n1d-grid로 볼 수 있음.(non-euclidean vs euclidean 참고)\n\nunderliying function이 regular graph로 정의(regular graph 참고)\n\n\n\n\nFigure: First example of GODE\n\n\n\n\nEsmaple 2. Orbit\nData Information\n\\({\\cal G}_W=(V,E,{\\bf W})\\)\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}:=\\{(x_1,y_2,z_3),\\dots,(x_n,y_n,z_n)\\}\\)\n\n\\(r_i= 5 + \\cos(\\frac{12\\pi (i - 1)}{n - 1})\\), \\(\\theta_i= -\\pi + \\frac{{\\pi(n-2)(i - 1)}}{n(n - 1)}\\)\n\\(x_i = r_i \\cos(\\theta_i)\\), \\(y_i = r_i \\sin(\\theta_i)\\), \\(z_i = 10 \\cdot \\sin(\\frac{{6\\pi \\cdot (i - 1)}}{{n - 1}})\\).\n\n\\(W_{i,j} = \\begin{cases} \\exp\\left(-\\frac{\\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2}{2\\theta^2}\\right) & \\|{\\boldsymbol v}_i -{\\boldsymbol v}_j\\|^2 \\le \\kappa \\\\ 0 & o.w\\end{cases}\\)\n\nNon-Euclidean Data\n\n2D shape이다.\ndomain이 곡선인 논유클리디안 데이터\nunderlying function이 regular graph로 정의되지 않는다.(regular graph 참고)\n거리 계산을 유클리드 거리로 할 때, underline이 곡선이라 합리적이지 않다.\nweight은 유클리디안 거리로 정의되어 있지만, \\(\\kappa\\)로 hyperparameter 지정해주어 거리가 짧으면 유클리디안 거리로 정의하고, 멀면 0으로 연결을 끊는 행렬으로 정의.\n\n\n\n\n\n\n\nNote\n\n\n\n유클리디안으로 보고 싶다면??\n\n모든 연결 weight를 끊어버리면 된다.\n그러면 regular graph 로 정의 가능.\n하지만, d연구의 목적에 어긋남.\n\n\n\n\nNote that \\({\\bf W}\\) is GSO, since \\({\\bf W}^\\top = {\\bf W}\\). In this cases, \\({\\cal G}_W\\) is not regular since there does not exist \\(k\\) such that \\({\\bf D}=k{\\bf I}\\).\n\ngraph signal \\({\\bf y}:V \\to \\mathbb{R}^3\\)\n\n\\({\\bf y}_i={\\boldsymbol v}_i+{\\boldsymbol \\eta}_i+{\\boldsymbol \\epsilon}_i\\)\n\\({\\boldsymbol \\epsilon}_i \\sim N({\\boldsymbol \\mu},\\sigma^2{\\bf I})\\)\n\\({\\boldsymbol \\eta}_i \\sim U^\\star\\) with sparsity \\(0.05\\) where \\(U^\\star\\) is mixture of \\(U(5,7)\\) and \\(U(-7,-5)\\).\n\n\nClearly \\({\\bf y}\\) is stationary w.r.t. \\({\\bf W}\\) or \\({\\bf L}\\).\n\n\n\n\nFigure: Second example of GODE\n\n\n\n\nEsmaple 3. Stanford Bunny\nData Information\npygsp 라이브러리를 사용하여 데이터 가져옴, weight도!(mesh 로 색의 퍼짐 정도로 나타낸 것으로 간단 이해)\nNon-Euclidean Data\n\n3D shape\ndomain이 곡면이고, underline이 곡면인 넌유클리디안 데이터\nunderlying function 이 색\n\n아래를 설명해보자면, 연한 파란-연한 초록 점이 언더라인 펑션으로 형성되어 있을때, 진한 색의 점들이 noise로 형성되어 있음.\n\nunderline이 곡면이라 유클리디안 거리를 사용하는 것이 합리적이지 않다.\n\n\n\n\nFigure: Third example of GODE\n\n\n\n\nReal data. Earthquake\nData Information\nThis data is actual data collected from USGS1 during the period from 2010 to 2014.1 https://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics\n\\({\\cal G}=(V,E)\\)\n\n\\(V=\\{{\\boldsymbol v}_1,\\dots,{\\boldsymbol v}_n\\}\\) where \\({\\boldsymbol v}:=({\\tt Latitude},{\\tt Longitude})\\)\n\\(W_{i,j} = \\begin{cases} \\exp(-\\frac{\\rho(i,j)}{2\\theta^2}) & if \\rho(i,j) \\le \\kappa \\\\ 0 & o.w. \\end{cases}\\)\n\nHere, \\(\\rho(i,j)=hs({\\boldsymbol v}_i, {\\boldsymbol v}_j)\\) is Haversine distance between \\({\\boldsymbol v}_i, {\\boldsymbol v}_j\\).\n\n\\(y_i\\) = magnitude\n\nNon-Euclidean Data\n\ndomain이 곡면이고, underline이 곡면인 넌유클리디안 데이터\nunderlying function 이 magnitude(지진 강도)\nhaversine 이용하면 곡면 거리가 이미 포함되어 있지만, 자료가 너무 많아 거리가 먼 연결을 끊어주는 역할로 hyperparameter인 \\(\\kappa\\)사용하였다.\n\n\nwith open(\"Figs/GODE_earthquake.pkl\", \"rb\") as file:\n    loaded_object = pickle.load(file)\n\nloaded_object"
  },
  {
    "objectID": "posts/3_Researches/GODE/2023-05-20-EbayesThresh toy ex.html",
    "href": "posts/3_Researches/GODE/2023-05-20-EbayesThresh toy ex.html",
    "title": "EbayesThresh Toy ex",
    "section": "",
    "text": "Import\n\nfrom itstgcn.learners import * \n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd\n\n\nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nWhile \\({\\bf p}_y\\) serves as a consistent estimator for \\(\\mathbb{E}[|{\\bf V}^H{\\bf y}|^2]\\), it is not an efficient estimator, and therefore, improvement is needed [@djuric2018cooperative]. The traditional approach for improvement is to use the windowed periodogram.\nThe windowed periodogram is efficient in detecting specific frequencies or periods, but it may not be as efficient in estimating the underlying function. One notable paper that utilized the windowed periodogram is the one that detected the El Niño phenomenon.\nAs this structure exhibits a “sparse signal + heavy-tailed” characteristics, by applying Bayesian modeling and thresholding \\({\\bf p}_y\\), we can estimate an appropriate \\({\\bf p}_{pp}\\) as discussed in [@johnstone2004needles].\n\n\n\nBayesian Model\n\\(x_i \\sim N(\\mu_i,1)\\)\n확률변수가 잘 정의되어 있을때, 여기서 \\(\\mu_i\\)를 정하는 Baysian.\n\n\\(\\mu_i \\sim\\) 사전분포(\\(\\mu_i\\)를 뽑을 수 있는)\n\\((\\mu_i | X_i = x_i)^n_{i=1} \\sim\\) 사후분포\n\nex) \\(N(10,1) \\sim\\) 사전분포\n관측치\n\n_obs = [7.1,6.9,8.5]\n\n\nnp.mean(_obs)\n\n7.5\n\n\n관측치를 보니 평균이 10이 아닌 것 같다.\n\\(N(10-3,1) \\sim\\) 사후분포\n\n여기서, \\(10-3\\)이 posterior meman\n사후 분포를 정의할때, 이벤트의 mean이냐, median이냐로 잡는 방법은 정해진 것이 아니다.(이베이즈에서는 median으로 잡음)\n\nEbayes는 사전분포를 Heavy-tail으로 정의했다.\nheavy tail?\n\n\n\nimage.png\n\n\n\\(\\mu_x \\sim\\) Double Exponential \\(= p_{pp} + p_{ac}\\) -> 혼합형(misture) = pure point + absolutely continuous\n\\(E(\\mu_i | X_i = x_i) = \\hat{\\mu}_i\\) -> thresholding의 결과\n\\(f_{prior}(\\mu) = (1-w)\\delta_0(\\mu) + w \\gamma (\\mu)\\)\n\n\\(\\delta_0\\) = 디렉함수(특정값이 아니면 다 0으로 봄)\n\\(\\gamma = \\frac{a}{2} e^{-a|\\mu|}\\)\n\nEbayes의 역할 = 자동으로 \\(w\\)를 계산 혹은 추정\n\n\\(1-w\\) 확률로 \\(\\delta_0\\)를 정의, \\(w\\)의 확룔로 \\(\\gamma\\)를 정의.\n\n\\(X_i = \\mu_i + \\epsilon_i, \\epsilon_i \\sim N(0,1)\\)에서 \\(\\mu_i\\)를 찾는게 목적이다. 이게 바로 \\(\\eta\\)값\n\nEbayes로 sparse signal만 골러낼 것이다.\n평균 이상의 값에서 자를 것이다.\n\nthresh(임계치)를 잡는 게 어려울 텐데, 위에서 이베이즈가 \\(w\\)를 자동으로 잡아 확률 계산되는 방법론을 제안한 것,\n\nbaysian modeling 사용하여 heavy tail + impulse(sparse)에서 posterior median 추정하여 임계값thresh으로 \\(p\\)에서 \\(p_{pp}\\)를 추출하는 것이 GODE 목적\n\n\n\nEbayesThresh\n\nT = 100\n\n\nt = np.arange(T)/T * 10\n\n\ny_true = 3*np.sin(0.5*t) + 1.2*np.sin(1.0*t) + 0.5*np.sin(1.2*t) \n\n\ny = y_true + np.random.normal(size=T)\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y_true)\n\n\n\n\n- 관찰한 신호\n\nplt.plot(t,y,'o')\nplt.plot(t,y_true,'--')\n\n\n\n\n- 퓨리에 변환\n\nf = np.array(y)\nif len(f.shape)==1: f = f.reshape(-1,1)\nT,N = f.shape\nPsi = make_Psi(T)\nfbar = Psi.T @ f # apply dft \n\n\nplt.plot(t,fbar**2) # periodogram \n\n\n\n\n- threshed\n\nfbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\nplt.plot((fbar**2)) # periodogram \nplt.plot((fbar_threshed**2)) \n\n\n\n\n\nplt.plot((fbar**2)[20:80]) # periodogram \nplt.plot((fbar_threshed**2)[20:80]) \n\n\n\n\n- 역퓨리에변환\n\nyhat = Psi @ fbar_threshed # inverse dft\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y,'.')\nplt.plot(t,y_true,'--')\nplt.plot(t,yhat)\n\n\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(y,'.')\nplt.plot(y_true)\n\n\n\n\n\n\nResult\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(40,15))\n    fig.suptitle('Figure 1',fontsize=40)\n    \n    ax1.plot(y, 'b.',alpha=0.5)\n    ax1.plot(y_true,'p--',label='True')\n    ax1.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    \n    ax1.tick_params(axis='y', labelsize=20)\n    ax1.tick_params(axis='x', labelsize=20)\n    \n    ax2.plot(y, 'b.',alpha=0.5)\n    ax2.plot(y_true,'p--',label='True')\n    ax2.plot(yhat,label='y hat')\n    ax2.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot((fbar**2)) # periodogram \n    ax3.plot((fbar_threshed**2)) \n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    ax3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n    \n    ax4.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \n    ax4.plot(range(20, 80),(fbar_threshed**2)[20:80]) \n    ax4.set_xticks(range(20, 81, 10))\n    ax4.set_xticklabels(range(20, 81, 10))\n    # ax4.set_xticklabels(['20','40','60'])\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n\n\n\n\n\nfrom mpl_toolkits.axes_grid1.inset_locator import mark_inset, inset_axes\nplt.figure(figsize = (20,10))\nplt.suptitle('Figure',fontsize=40)\nax = plt.subplot(1, 1, 1)\nax.plot(range(0,100),(fbar**2))\nax.plot((fbar_threshed**2)) \naxins = inset_axes(ax, 8, 3, loc = 1, bbox_to_anchor=(0.8, 0.8),\n                   bbox_transform = ax.figure.transFigure)\naxins.plot(range(20, 80),(fbar**2)[20:80])\naxins.plot(range(20, 80),(fbar_threshed**2)[20:80]) \naxins.set_xlim(20, 80)\naxins.set_ylim(-0.1, 7)\nmark_inset(ax, axins, loc1=4, loc2=3, fc=\"none\", ec = \"0.01\")\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='x', labelsize=20)\naxins.tick_params(axis='y', labelsize=15)\naxins.tick_params(axis='x', labelsize=15)\n# plt.savefig('Ebayes_Toy.png')\n\n\n\n\n\nfrom matplotlib.patches import ConnectionPatch\nfig = plt.figure(figsize=(20,10))\nplt.suptitle('Figure 1',fontsize=40)\nplot1 = fig.add_subplot(2,2,(1,2))\n\nplot1.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \nplot1.plot(range(20, 80),(fbar_threshed**2)[20:80]) \nplot1.set_xticks(range(20, 81, 10))\nplot1.set_xticklabels(range(20, 81, 10))\nplot1.tick_params(axis='y', labelsize=20)\nplot1.tick_params(axis='x', labelsize=20)\n\nplot3 = fig.add_subplot(2,2,(3,4)) \n\nplot3.plot((fbar**2)) # periodogram \nplot3.plot((fbar_threshed**2)) \nplot3.tick_params(axis='y', labelsize=20)\nplot3.tick_params(axis='x', labelsize=20)\nplot3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n# plot3.fill_between((20, 80), 10, 60, facecolor= \"red\", alpha = 0.2)\nconn1 = ConnectionPatch(xyA = (20, -0.1), coordsA=plot1.transData,\n                       xyB=(20, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn1)\nconn2 = ConnectionPatch(xyA = (79, -0.1), coordsA=plot1.transData,\n                       xyB=(80, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn2)\nplt.show()\n\n\n\n\n\n\nIn article\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nimport rpy2\n\n\nfrom rpy2.robjects.packages import importr\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n#import rpy2\n#import rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nGNAR = importr('GNAR') # import GNAR \n#igraph = importr('igraph') # import igraph \nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n%%R\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n\n\\(X_i\\)에서 \\(\\mu_i\\) 추출 가능하는 것을 증명할 예제\n\n%%R\n# png(\"Ebayes_plot1.png\", width=1600, height=800)\npar(mfrow=c(1,2))\npar(cex.axis=2) \npar(cex.lab=2)\nplot(x, type='l', xlab=\"Observed data\", ylab=\"\")\nplot(ebayesthresh(x, sdev=1),type='l', xlab=\"Estimate\", ylab=\"\")\n# dev.off()\n\n\n\n\n\nimport itstgcn\n\n\nitstgcn.make_Psi(T)\n\narray([[ 0.07106691, -0.10050378,  0.10050378, ..., -0.10050378,\n        -0.10050378,  0.07106691],\n       [ 0.10050378, -0.14206225,  0.14184765, ...,  0.14184765,\n         0.14206225, -0.10050378],\n       [ 0.10050378, -0.14184765,  0.14099032, ..., -0.14099032,\n        -0.14184765,  0.10050378],\n       ...,\n       [ 0.10050378,  0.14184765,  0.14099032, ...,  0.14099032,\n        -0.14184765, -0.10050378],\n       [ 0.10050378,  0.14206225,  0.14184765, ..., -0.14184765,\n         0.14206225,  0.10050378],\n       [ 0.07106691,  0.10050378,  0.10050378, ...,  0.10050378,\n        -0.10050378, -0.07106691]])\n\n\ndef trim(f):\n    f = np.array(f)\n    if len(f.shape)==1: f = f.reshape(-1,1)\n    T,N = f.shape\n    Psi = make_Psi(T)\n    fbar = Psi.T @ f # apply dft \n    fbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\n    fhat = Psi @ fbar_threshed # inverse dft \n    return fhat\n\nplt.plot(y)\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T).T@y)\n\n\n\n\n\nplt.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T)@ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\n_T = 1000\n\n\n_t = np.arange(_T)/_T * 10\n\n\n_x = 1.5*np.sin(2*_t)+2*np.random.rand(_T)+1.5*np.sin(4*_t)+1.5*np.sin(8*_t)\nplt.plot(_x)\n\n\n\n\n\nimport itstgcn\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\n_node_ids = {'node1':0,'node2':1}\n\n_FX1 = np.stack([_x,_x],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n\ndata1 = pd.DataFrame({'x':_x,'x1':_x,'xer':_x,'xer1':_x})\n\n\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset = loader1.get_dataset(lags=2)\n\n\nmindex = itstgcn.rand_mindex(dataset,mrate=0.7)\ndataset_miss = itstgcn.miss(dataset,mindex,mtype='rand')\n\n\ndataset_padded = itstgcn.padding(dataset_miss,interpolation_method='linear')\n\n\nlrnr = itstgcn.StgcnLearner(dataset_padded)\n\n\nlrnr.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor = Eval_csy(lrnr,dataset_padded)\n\n\nlrnr_2 = itstgcn.ITStgcnLearner(dataset_padded)\n\n\nlrnr_2.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor_2 = Eval_csy(lrnr_2,dataset_padded)\n\nPsi\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fst.pdf', format='pdf')\n\n\n\n\nfourier transform\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem(itstgcn.make_Psi(_T).T@np.array(_x),linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem((itstgcn.make_Psi(_T).T@np.array(_x))[:100],linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd_zin.pdf', format='pdf')\n\n\n\n\nEbayesthresh/trim\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem((ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))))[:100],linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd_zout.pdf', format='pdf')\n\n\n\n\nfhat\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3,alpha=0.3)\n    ax1.plot(itstgcn.make_Psi(_T)@ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='Inverse Fourier Transform',lw=5)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fth.pdf', format='pdf')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax1.plot(data1['x'][:],'-',color='C3',label='Complete Data')\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_fst.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax2 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax2.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax2.plot(torch.cat([torch.tensor(data1['x'][:4]),torch.tensor(dataset_miss.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data',markersize=15)\n    ax2.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=40)\n    ax2.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_snd.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax3 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)    \n    ax3.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(evtor_2.f_tr[:,0],'--o',color='C3',alpha=0.8,label='Interpolarion')\n    ax3.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=40)\n    ax3.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_3rd.png')\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax4 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax4.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(evtor.fhat_tr[:,0],color='brown',lw=3,label='STGCN')\n    ax4.plot(evtor_2.fhat_tr[:,0],color='blue',lw=3,label='ITSTGCN')\n    # ax4.plot(138, -1.2, 'o', markersize=230, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(220, -1.5, 'o', markersize=200, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(290, -1.2, 'o', markersize=310, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(455, -0.9, 'o', markersize=280, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=40)\n    ax4.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_4th_1.png')"
  },
  {
    "objectID": "1_essays.html",
    "href": "1_essays.html",
    "title": "Essays",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "2_graph.html",
    "href": "2_graph.html",
    "title": "Graph",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 2, 2023\n\n\nGraph Shift Operator\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nNon-Euclidean vs Euclidean\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nGraph Signal\n\n\nSEOYEON CHOI\n\n\n\n\nJun 30, 2023\n\n\nRegular Graph\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was generated to manage my blogs.\nThere are two purposes to achieving something here.\n\nFirst\nI will get definitions of any subjects and organize that in my words.\nIt is important to understand those definitions if I want to research exactly and improve myself.\n\n\nSecond\nI will brief my past and present researches.\nOverall, this blog will be the essential step in developing my skills."
  },
  {
    "objectID": "3_itstgcn.html",
    "href": "3_itstgcn.html",
    "title": "ITSTGCN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 18, 2023\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  }
]